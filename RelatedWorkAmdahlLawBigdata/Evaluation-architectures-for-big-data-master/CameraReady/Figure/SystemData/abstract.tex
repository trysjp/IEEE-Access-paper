%Efficient processing of bigdata produced by different scientific experimental facilities poses several challenges in terms of efficient storage, transfer, in-memory computation and exploiting locality of these data. 
%The existing supercomputers being optimally tuned for high performance compute intensive applications like MPI, Grid etc cannot address most of these challenges especially due to their storage and memory constraint.
%On the other hand, in the last decade, Hadoop and other state of the art bigdata analytics softwares shifted the existing model of computation severely towards data driven analysis creating new oportunity to address many HPC challenges involving bigdata.
%Although, these massively parallel bigdata analytics softwares made several HPC computation easy, efficient provisioning in terms hardwares still remain a major challenge especially in the context of data intensive high performance computation.

%In this paper we juxtapose different distributed cyber infrastructure including traditional supercomputers and available cloud resources with Intel hiBench and our bigdata Parallel Genome Assembler (PGA) based upon Hadoop and Giraph, that serves as a very good example of data as well as compute intensive application.
%We compare the perfromance of LSU supercomputer QueenBeeII and two different types of clusters located in Samsung Korea with our assembler and observed Samsung Cluster with [SSD and 10GigE....] outperformed QueenBeeII in terms of [price to performance....] because of [high processor density, higher IO throughput....].
%Unlike other studies, our analysis is not restricted in comparing any single system component (eg. storage or network interconnection) rather motivated to answer a higher level question that is becoming increasingly important 'how does the next generation bigdata (tera/petabyte scale) high performance computation cluster should look like'
The enormous growth of bigdata produced by different experimental facilities is rapidly changing the model of computation in the domain of high performance computing (HPC).
Many HPC aficionados, in order to efficiently manage their data intensive workload started using the current state of the bigdata analytics softwares like Hdoop, giraph etc. devieting from traditional parallel programming models like MPI etc.
%Unprecedented amount of bigdata produced by different advanced scientific experimental facilities make the high performance computation a severe data intensive endeavor.
%In order to address the challnges involved in efficient storage, management and analysis of these bigdata, recently, many HPC aficionados started using the current state of the art bigdata analytics softwares like Hadoop, Giraph etc for their data intensive scientific workloads diviating from traditional compute-intensive programming paradigm like MPI, Grid etc.
Consequently, traditional supercomputers with lots of processing power are found to provide suboptimal performance due to several limitations in the underlying hardware infrastructure creating new opportunities for hardware manufacturers as well as the cloud service providers.

In this paper, we compare the performance of a traditional suprcomputer named SuperMikeII located in LSU and our state of the art bigdata analytics cluster called SwatIII located in Samsung, Korea) while handling a data-intensive workload. and show how we achieve x-speedup using only 1/x nodes and subsequently x\% performance to price benefit in SwatIII.
%There is limited understanding on the performance characteristics of the state of the art bigdata analytics softwares on different type of distributed cyberinfrastructure when applied for data intensive scientific workload.
Our analysis is based upon our benchmark large-scale Parallel Genome Assembler (PGA) based upon Hadoop and Giraph. 
Our assembly pipeline consists of a huge amount of short read analysis using Hadoop, followed by a large de Bruijn graph analysis using Giraph, thus serving as a very good real world example of data as well as compute intensive workload.
%We evaluate the performance of our assembler atop LSU supercomputer, SuperMikeII and two different types of state of the art bigdata analytics clusters, SwatIII and CeresII located in Samsung Korea. 
%Our analysis provides insights on the actual limitations of tradional supercomputers as well as pinpoint various design considerations and performance-tradeoffs from the perspective of efficient hardware provisioning.
%We show how we achieve almost x-speedup and y\% performance to price benefit in our state of the art cluster than a traditional supercomputer.