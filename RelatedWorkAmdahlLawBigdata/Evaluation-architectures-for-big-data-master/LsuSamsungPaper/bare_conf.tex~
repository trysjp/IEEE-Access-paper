
%% bare_conf.tex
%% V1.3
%% 2007/01/11
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.7 or later) with an IEEE conference paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%
%% File list of work: IEEEtran.cls, IEEEtran_HOWTO.pdf, bare_adv.tex,
%%                    bare_conf.tex, bare_jrnl.tex, bare_jrnl_compsoc.tex
%%*************************************************************************

% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. IEEE's font choices can trigger bugs that do  ***
% *** not appear when using other class files.                            ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



% Note that the a4paper option is mainly intended so that authors in
% countries using A4 can easily print to A4 and see how their papers will
% look in print - the typesetting of the document will not typically be
% affected with changes in paper size (but the bottom and side margins will).
% Use the testflow package mentioned above to verify correct handling of
% both paper sizes by the user's LaTeX system.
%
% Also note that the "draftcls" or "draftclsnofoot", not "draft", option
% should be used if it is desired that the figures are to be displayed in
% draft mode.
%
\documentclass[conference]{IEEEtran}
% Add the compsoc option for Computer Society conferences.
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[conference]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/tex-archive/macros/latex/contrib/oberdiek/
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 4.0 (2003-05-27) and later if using hyperref.sty. cite.sty does
% not currently provide for hyperlinked citations.
% The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/cite/
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at: 
% http://www.ctan.org/tex-archive/macros/latex/required/graphics/
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found as epslatex.ps or
% epslatex.pdf at: http://www.ctan.org/tex-archive/info/
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage[cmex10]{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics. If using
% it, be sure to load this package with the cmex10 option to ensure that
% only type 1 fonts will utilized at all point sizes. Without this option,
% it is possible that some math symbols, particularly those within
% footnotes, will be rendered in bitmap form which will result in a
% document that can not be IEEE Xplore compliant!
%
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/amslatex/math/





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithms/
% There is also a support site at:
% http://algorithms.berlios.de/index.html
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithmicx/




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/


%\usepackage{mdwmath}
%\usepackage{mdwtab}
% Also highly recommended is Mark Wooding's extremely powerful MDW tools,
% especially mdwmath.sty and mdwtab.sty which are used to format equations
% and tables, respectively. The MDWtools set is already installed on most
% LaTeX systems. The lastest version and documentation is available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/mdwtools/


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.


%\usepackage{eqparbox}
% Also of notable interest is Scott Pakin's eqparbox package for creating
% (automatically sized) equal width boxes - aka "natural width parboxes".
% Available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/eqparbox/





% *** SUBFIGURE PACKAGES ***
%\usepackage[tight,footnotesize]{subfigure}
% subfigure.sty was written by Steven Douglas Cochran. This package makes it
% easy to put subfigures in your figures. e.g., "Figure 1a and 1b". For IEEE
% work, it is a good idea to load it with the tight package option to reduce
% the amount of white space around the subfigures. subfigure.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at:
% http://www.ctan.org/tex-archive/obsolete/macros/latex/contrib/subfigure/
% subfigure.sty has been superceeded by subfig.sty.



%\usepackage[caption=false]{caption}
%\usepackage[font=footnotesize]{subfig}
% subfig.sty, also written by Steven Douglas Cochran, is the modern
% replacement for subfigure.sty. However, subfig.sty requires and
% automatically loads Axel Sommerfeldt's caption.sty which will override
% IEEEtran.cls handling of captions and this will result in nonIEEE style
% figure/table captions. To prevent this problem, be sure and preload
% caption.sty with its "caption=false" package option. This is will preserve
% IEEEtran.cls handing of captions. Version 1.3 (2005/06/28) and later 
% (recommended due to many improvements over 1.2) of subfig.sty supports
% the caption=false option directly:
%\usepackage[caption=false,font=footnotesize]{subfig}
%
% The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/subfig/
% The latest version and documentation of caption.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/caption/




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure. The latest version and documentation can be found at:
% http://www.ctan.org/tex-archive/macros/latex/base/



%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/sttools/
% Documentation is contained in the stfloats.sty comments as well as in the
% presfull.pdf file. Do not use the stfloats baselinefloat ability as IEEE
% does not allow \baselineskip to stretch. Authors submitting work to the
% IEEE should note that IEEE rarely uses double column equations and
% that authors should try to avoid such use. Do not be tempted to use the
% cuted.sty or midfloat.sty packages (also by Sigitas Tolusis) as IEEE does
% not format its papers in such ways.





% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/misc/
% Read the url.sty source comments for usage information. Basically,
% \url{my_url_here}.

\usepackage{paralist}



% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{Efficient Provisioning for Bigdata HPC: A System Approach}


% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{\IEEEauthorblockN{Arghya Kusum Das, Seung Jong Park}
\IEEEauthorblockA{School of Electrical and\\Computer Engineering\\
Louisisna State University\\
Baton Rouge, LA, 70801 \\
Email: adas7, sjpark @lsu.edu}
%\and
%\IEEEauthorblockN{Homer Simpson}
%\IEEEauthorblockA{Twentieth Century Fox\\
%Springfield, USA\\
%Email: homer@thesimpsons.com}
%\and
%\IEEEauthorblockN{James Kirk\\ and Montgomery Scott}
%\IEEEauthorblockA{Starfleet Academy\\
%San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212\\
%Fax: (888) 555--1212}
}

% conference papers do not typically use \thanks and this command
% is locked out in conference mode. If really needed, such as for
% the acknowledgment of grants, issue a \IEEEoverridecommandlockouts
% after \documentclass

% for over three affiliations, or if they all won't fit within the width
% of the page, use this alternative format:
% 
%\author{\IEEEauthorblockN{Michael Shell\IEEEauthorrefmark{1},
%Homer Simpson\IEEEauthorrefmark{2},
%James Kirk\IEEEauthorrefmark{3}, 
%Montgomery Scott\IEEEauthorrefmark{3} and
%Eldon Tyrell\IEEEauthorrefmark{4}}
%\IEEEauthorblockA{\IEEEauthorrefmark{1}School of Electrical and Computer Engineering\\
%Georgia Institute of Technology,
%Atlanta, Georgia 30332--0250\\ Email: see http://www.michaelshell.org/contact.html}
%\IEEEauthorblockA{\IEEEauthorrefmark{2}Twentieth Century Fox, Springfield, USA\\
%Email: homer@thesimpsons.com}
%\IEEEauthorblockA{\IEEEauthorrefmark{3}Starfleet Academy, San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212, Fax: (888) 555--1212}
%\IEEEauthorblockA{\IEEEauthorrefmark{4}Tyrell Inc., 123 Replicant Street, Los Angeles, California 90210--4321}}




% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle


\begin{abstract}
%\boldmath
%The abstract goes here. 
%Recently, Hadoop and the rich software ecosystem that is built surrounding it became the defacto statndard of distributed computing involving bigdata.
%In the last few years, their performance is well explored for several data intensive enterprise application including log processing, machine learning, indexing etc.
%On the other hand, it is hardly explored for scientific application which involves terabyte scale data (eg. genome assembly) which are traditionally addressed by HPC, however, their flexible programming model can address many of them.
%Unlike the enterprise appliction these are highly tuned for existing supercomputers which differs from the scaledout clusters of commodity hardware in many aspect including CPU (in terms of FLOP), network (infiniband and ethernet) etc. However Hadoop initially targetted the commodity clusters, intersting it supports many of the supercomputing aspect also. 
%Not only they perform well over infifiniband, recent studies showed their io throughput can be incresed heavily using solid state drives (SSD) which is several magnitudes faster than normal hard disk drive (HDD).
%This flexible programming model offered by these bigdata analytics sofetware and their support for varying range of infrastructure creates a growing interest in terms of efficient provisioning of hardware which creates a gap between two otherwise convrging field, Bigdata Analytics and High Performance Computing

%In this  paper we discuss about ebout experimenting our parallel Hadoop and Giraph based genome assembler (PGA) that we developed to address the challenges involved in largesacle genome assembly, an HPC problem that recently made its way to the forefront of bigdata.
%We juxtapose current state of the art big data analysis software (Hadoop and Giraph) and available distributed computing resources with our assembler that serves as a very good example of both data as well as computation intensive job.
%We performed a series of experiments on different types of clusters including LSU supercomputers and Samsung clusters which vary in terms of storage (SSD and HDD), network (IB and Ethernet), processor density, available memory etc.
%we present a detail result and analysis of performance of PGA on these clusters in terms of io, cpu and memory utilization and networking. 
%We believe it will serve as one of the initial but solid foundation to answer 'how does the next generation high performance computation cluster to handle bigdata (tera/petabyte scale) should look like'



%In the last few years Hadoop and the rich software ecosystem that is built surrounding it became the de facto standard for the distriuted computing involving bigdata (tera/petabyte scale).
%The flexible programming model offered by the current state of the art bigdata analysis software (Hadoop, Giraph etc.), and their support for varying range of distributed cyber infrastructure including different types of storage (SSD and HDD) and network architecture (Infiniband and Ethernet) creates a growing interest in the research community to apply them for HPC challenges.
%Despite the growing interest, efficient provisioning and fine tuning these massively distributed system still remain a major challenge, especially in the context of high performance computing.
%On the other hand, most of the recent studies that analyze Hadoop's performance on differnet infrastructure, are limited to the enterprise data analytics jobs only (eg. log processing, sorting etc.),  that take less than 100GB of input [Microsoft][ChenFB], thereby ignored the hpc aspect of bigdata entirely both in terms of complex computation as well as huge in-memory processing.
%Although, several recent studies [Microsoft][IBROCEcomp][HiTune][SSDFlashKorea] analyze the performance of Hadoop on differnet types of clusters, these studies being limited to the enterprise data analytics job only that take less than 100GB of input [Microsoft][ChenFB], ignores the hpc aspect of bigdata entirely.
%Furthermore, they use the existing benchmarks (eg [hibench]) which are not made reflect the behavior of HPC problems which includes complex algorithms and highly tuned for the existing supercomputers.
%In this paper we discuss about experimenting PGA, a Hadoop and Giraph based Parallel Genome assembler that we developed to address the challenegs involved in large scale genome assembly, an HPC application that recently made its way to the forefront of bigdata.
This paper is motivated by the growing demand for developing comprehensive next generation distributed cyberinfrastructure in support of bigdata high-performance scientific and engineering applications.
The flexible programming model offered by the existing opensource state of the art bigdata softwares, and their support for different cyberinfrastructure both in terms of storage (SSD and HDD) and network (Ethernet and infiniband) made it possible to use them for HPC application which posed catalytic effect on this demand. %however, the research seems to be hindered by unavailability of proper benchmark or some examplary software.
In this paper, we consider large scale genome assembly as an example HPC problem that recently made its way to the forefront of bigdata. 
We juxtapose the current state of the art big data analytics software and the available distributed computing resources with our Parallel Hadoop and Giraph based bigdata Genome Assembler, PGA,  that serves as a very good example of both data as well as computation intensive job.
PGA successfully assembled large scale mammalian genome including human genome of size 450GB that produces almost 4TB intermidiate de Bruijn graph [pevzner] in LSU supercomputers and clusters located in Samsung, Korea.
We compare PGA's performance on different types of clusters and found that the Samsung cluster with SSD outperformed the supercomputers due to their higher IO throughput, higher memory availability and high processor density. %Also our cost analysis shows the . %storage (SSD and HDD) as well as network architecture (Infiniband and Ethernet).
We believe, the performance results of PGA and our detailed analysis will serve as one of the initial however, solid foundation to answer the question that is becoming increasingly important 'how does the next generation high performance computation cluster to handle bigdata (tera/petabyte scale) should look like'


\end{abstract}
% IEEEtran.cls defaults to using nonbold math in the Abstract.
% This preserves the distinction between vectors and scalars. However,
% if the conference you are submitting to favors bold math in the abstract,
% then you can use LaTeX's standard command \boldmath at the very start
% of the abstract to achieve this. Many IEEE journals/conferences frown on
% math in the abstract anyway.

% no keywords




% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction}
% no \IEEEPARstart
%This demo file is intended to serve as a ``starter file''
%for IEEE conference papers produced under \LaTeX\ using
%IEEEtran.cls version 1.7 and later.
% You must have at least 2 lines in the paragraph with the drop letter
% (should never be an issue)
%I wish you the best of success.

Efficient processing of big data is important for modern analytics that led the development of several bigdata analytics frameworks in the last few years, especially after the introduction of Hadoop, the opensource map-reduce framework that became the enterprise de-facto standard of distributed computing and revolutionized our ability to find signals in noise.
%However, these analytics software makes it easy to develop and run highly scalable application, efficient provisioning in terms of storage, memory or network still remain a major challenge.
%On the otherhand, with the recent advances in scientific instruments many of the HPC problems recently made their way to the forefront of bigdata challenges.
%One such example is de novo genome assembly which maps the assembly of short read sequences as a graph (de bruijn Graph [pevzner]) analysis problem and conventionally addressed by HPC only. 
%Recent advances in massively parallel high throughput DNA sequencer like Illumina Genome Analyzer etc. produce vast amount of high quality short read sequences which generate good quality contigs with high coverage when assembled de novo.
On the other hand, the flexible programming model offered by these state of the art bigdata analytics software (Hadoop, Giraph etc.), and their support for varying range of distributed cyber infrastructure including different types of storage (SSD and HDD) and network architecture (Infiniband and Ethernet) creates a growing interest in the research community to apply them for HPC challenges.
Despite the growing interest, efficient provisioning of these massively distributed system still remain a major challenge, especially in the context of high performance computing.
%This creates a growing interest in the HPC community to apply these state of the art bigdata analytics softwares for HPC challenges.
%The new computation model provided by Hadoop and other bigdata analytics software is hardly explored for this type of scientific applications, although, they have enough potential to address these challenges efficiently, not only due to their flexible programming model, but also due to their support for varying range of cyberinfrastructure including different types of storage (SSD and HDD) and networks (IB and ethernet).
%Despite of this growing interest, the research is severely hindered by the existing studies that analyze the performance of Hadoop on different cyberinfrastructure. 
Although, Hadoop and its performance on different cyberinfrastructure is well studied in the last few years, unfortunately, most of these studies are limited to the enterprise data analytics jobs only (eg. log processing, sorting etc.),  thereby ignored the hpc aspect of bigdata entirely both in terms of complex computation as well as huge data processing in memory. 
%These studies [Microsoft][IBROCEcomp][HiTune][SSDFlashKorea] being limited to the enterprise data analytics job only, ignores the hpc aspect of bigdata entirely.
From the view point of high performance scientific bigdata analysis we summarize the limitations in these studies as follows:
First, most of the experiments are performed on top of the existing benchmarks (eg. [hibench]) which are ill-suited to reflect the data intensive supercomputing workload.
Second, although, Hadoop targetted tera or petabyte scale data, most of these experiments are performed on subtera scale since almost 90\% of the real world enterprise analytics jobs handle less than 100GB of input data[Microsoft][chenfb]. However,  this is not true for most of the HPC appications involving bigdata. 
Third, graphs are a core part of most analytics workloads. Typically large scale graph analysis involves multiple iteration along with enormous number of random reads/writes. In order to, avoid long latency, graph processing is traditionally done in memory. Although Apache Giraph addressed many of the large scale graph challenges successfully, unfortunately, it's performance is hardly studied in any of the recent works.
Fourth, most of the studies analyze any single component of distributed computing. They either compare the performance of Hadoop on different kinds of storage (SSD or HDD)[ssdFlashKorea] or studied it on different types of network architecture [IBROCEComp], thereby ignored their cumulative effect on the application. However, the performance and efficiency of Hadoop can be severely constrained with the technology choices in both the components. It is particularly true when the cluster is based on new generation processors and SSD storage.

In order to address these limitations, in our work, we chose de novo genome assembly as a representative HPC problem that recently made its way to the forefront of bigdata. 
De novo genome assembly involves analysis of huge amount (hundreds of GB) of shortreads produced by recent massively parallel genome sequencers followed by a large terabyte scale graph analysis (de Bruijn graph [pevzner]) thereby making the entire assembly problem a very good representative of Data intensive supercomputing applications workload. 
In this paper, we present the performance result of our parallel Hadoop and Giraph based high performance bigdata genome assembler on different types of cluster with varying storage and network architecture. PGA successfully assembled large scale mammalian genome including human genome of size 450GB that produces an intermidiate de Bruijn garph of size almost 4TB in LSU supercomputer QueenBee as well as two different types of clusters located in Samsung, Korea. 
%In order to answer this, first we identified three major bottlenecks in today's supercomputers in terms of efficient hardware provisioning.
%1) Today's supercomputers with limited amount of memory and/or storage cannot address the bigdata challenges efficiently. 
%2) However, Hadoop targetted normal HDD, several recent studies pointed out Hadoop performs better atop SSD which is several magnitudes faster than normal HDD. Whereas, all the existing supercomputers being dadicated for compute intensive jobs are provided with normal HDD. 
%3) Hadoop and the related softwares in its ecosystem, initially being targetted for commodity hardware is not optimized for infiniband, which is default in all the supercomputers. The performance and efficiency of these softwares can be severely constrained with the network architecture and technology choices especially when the cluster is based on new generation processors and SSD storage.
%In our work we took a system approach to address these challenges in efficient provisioning. 
%We performed a series of experiment with our parallel de bruijn genome assembler based on Hadoop and Giraph in order to compare the performance of LSU supercomputer as well as two different types of clusters that are located Samsung, Korea.
%<We found (this particular) cluster performs better than other.....>
Our contribution can be stated as follows:
1) We developed a parallel de novo genome assembler (PGA) as a benchmark that represents an hpc problem involving bigdata. We use Hadoop (in particular, Hadoop2.3.0 which is yarn enabled and comes as opensource with Cloudera Manager 5.0.0) and Giraph (Apache Giraph-1.1.0) to develop PGA that assembles vast amount of short reads produced by the highthroughput DNA sequencing machines (eg. Illumina Genome Analyzer).
2) Unlike previous works that targetted enterprise level data intensive analytics job (eg. log processing, indexing etc.), we juxtapose the current state of the art big data analytics software and available distributed computing resources with our assembler that serves as a very good example of both data as well as computation intensive job. 
3) Besides Hadoop, we also analyzed the behavior of Giraph, the graph processing framework that we used for huge de Bruijn graph analysis, thereby address a major area of performance analysis that is overlooked in most of the studies
4) Unlike other works, we focus on both storage and network architecture of the cluster in order to evaluate the performance of Hadoop and Giraph with respect to our parallel genome assembler.
%5) We tuned several hadoop storage, memory and network related parameters (eg. blocksize, java heap space, number of parallel threads for reducer copy etc.) and studied their impact on the performance of our assembler on different types of infrastructure.
%1) However, Hadoop targetted normal HDD, several recent studies pointed out Hadoop performs better atop SSD which is several magnitudes faster than normal HDD.
%2) Hadoop and the related softwares in its ecosystem, initially being targetted for commodity hardware is not optimized for infiniband, which is default in all the supercomputers. Their performance and efficiency can be severely constrained with the network architecture and technology choices. It is particularly true when the cluster is based on new generation processors and SSD storage.
Our detailed result and in depth analysis of cumulative effect of storage and network architecture on the performance of bigdata application (genome assembly) help addressing challenges involved in developing comprehensive, balanced and flexible next generation distributed cyberinfrastructure (DCI) in support of science and engineering applications entangled with bigdata, thereby help deciding on 'how does a next generation high performance computation cluster should look like'.
The rest of the paper is organized as follows. Section 2 describes Hadoop and Giraph. In section 3 we discuss the overview of our genome assembler and how much data it handles. Section 3 describes the available resources. Finally in Section 5 we presented our result.

%Although, conventional wisdom directs us towards a scaled out cluster of commodity hardware to cheaply process large amount of data, Hadoop has enough potential for improved performance atop SSD which is a magnitude faster than normal HDD.
%Also, it supports infiniband  which is considered as a basic requirement for HPC.
%Furthermore, the flexible programming model offered by these softwares not only solves enterprise level data intensive problems including log processing, analytic queries etc. but also fits a broad range of compute intensive problems that is conventionally addressed by high performance computing (HPC) only.
%On the other hand, recent advances in both storage and networking and the corresponding drop in price, made the way for cloud computing as a promising alternative to supercomputing. 
%So, the fundamental question that is becoming increasingly important, is how optimally can we exploit the capability of the current state of the art bigdata software not only in terms of performance and cost on different types of cyber infrastructure but also in terms of type of application as well.
%Unfortunately, due to lack of proper benchmarks or examplary software on Hadoop, none of the existing studies addressed this question propely. Hence, there is a need for a comprehensive study to help deciding on next generation distributed cyberinfrastructure in support of science and engineering applications that handles large amount of data. 

%In order to find the answer to this, In this paper, we discuss about experimenting PGA, a Parallel Genome Assembler based on Hadoop and Giraph that we developed to address the challenges involved in large scale genome assembly which recently made its way to the forefront of big data challenges.
%We juxtapose the performance result of current state of the art bigdata software in different types of HPC and Cloud resources (both scaled out and scaled up) with our assembler that serves as a very good example of both data as well as compute intensive job.
%We believe our in-depth analysis provides a guideline on how much benefit an end user can expect from different types of clusters and most importantly helps both the HPC and Cloud community in deciding how a next generation cluster should look like.

%In the last decade with the introduction of Hadoop big data analytics became synonymous with deploying cheap commodity clusters.
%Although, this scaleout thinking worked well for plethora of real world enterprise problems including log processing, analytic query etc, it hinders the research on exploring the full cpability of Hadoop and the rich ecosystem that is built around it, which if used properly can address a broad set of scientific and engineering problems especially due to their flexible progeamming model.
%Furthermore, recent studies showed that the majority of real world analytic jobs process less than 100GB of input (ref-Microsoft-paper) whereas Hadoop and the related software in its ecosystem is developed to address the data in the scale of terabytes or petabytes.
%This suboptimal use motivates many HPC aficionados to observe big data analysis as a form of HPC despite of several fundamental differences in computation and communication characteristics involved in HPC and the current state of the art bigdata analytics softwares.
%On the other hand, the need for comprehensive, balanced and flexible distributed cyberinfrastructure (DCI) in support of science and engineering applications (ref-SCREAM) entangled with bigdata drove the related research community in the last few years like never seen before. 
%With the growing interest on this next generation DCI, it is etremely important to address the challenges involved in supporting broad range of application usage scenarios on a range of platforms with varying performance.

%\hfill mds
 
%\hfill January 11, 2007

%\subsection{Subsection Heading Here}
%Subsection text here.


%\subsubsection{Subsubsection Heading Here}
%Subsubsection text here.

\section {Bigdata software and HPC}
\subsection {Hadoop}
Hadoop map-reduce consists of three different phases called map, shuffle and reduce.
The input data is splitted into several disjoint sets called blocks and is fed to simultaneous map processes which executes a user-defined function parallely without interacting with each other.
This shared nothing architecture particularly helps embarassingly parallel processes to handle petabyte scale data and the root cause behind the behind the success of scaleout paradigm.
The mappers output, in the form of a key-value pair is written into an in memory buffer and once the buffer is full it is spilled to local disk.
These intermidiate key value pairs are then shuffled to the reducers based upon the key, which means, the values with the same key is shuffled to the same reducer.
Finally, the reducers execute a user-defined reduce function on the values with the same keys and write the final output to a distributed file system called HDFS.

Hadoop can be classified as a Single Instruction Multiple Data (SIMD) problems with a fault tollarent distributed storage called Hadoop Distributed File System (HDFS).
On the other hand, the traditional HPC supports both MIMD and SIMD but hardly provide any storage.
Hadoop clusters usually include a large amount of local disk space (used for HDFS nodes), whereas many HPC clusters rely on NFS or a parallel filesystem (eg. Lustre) for cluster-wide storage.
In most of the supercomputers, a separate group of hardware is often used for global file storage whereas the performance benefit of Hadoop comes from moving the computation to colocate with the data.
  
\subsection {Giraph}

\section{PGA Software Overview}
\subsection{De Novo Genome Assembly}
De novo genome assembly refers to construction of an entire genome sequence from a large amount of short read sequences when no reference genome is available. 
De Bruijn graph construction  and removal of read errors (tips and bubble) from this graph is central to de novo sequencing. 
Finally, resolving  repeated regions followed by a scaffolding phase produces long size scaffolds that represents a region in the actual genome.

\subsection{Architecture}
We classified de novo sequencing in three different phases.
\begin{inparaenum}[\itshape a\upshape)]
\item Building de Bruijn graph
\item Error removal from de Bruijn graph and
\item Scaffolding.
\end{inparaenum}
We store short reads in fastq format in hdfs as input to PGA.
In the first phase, we use Hadoop in order to build de Bruijn graph from these short reads. 
Once the graph is constructed we use Giraph in the subsequent phases to analyze the graph in order to construct appreciably long contigs and scaffolds.
We use Giraph for two distinct reasons.
\begin{inparaenum}[\itshape a\upshape)]
\item In-memory graph processing performs several magnitudes faster than disk based approach. However, the graph size is limited by the available memory.
\item Although, the vertex centric, synchronous graph processing model works slower than its asynchronous variation in many cases, it makes the de Bruijn graph analysis extremely easy. 
\end{inparaenum}

\section{Available Resources}
We test PGA on SuperMikeII an Samsung clusters.
SuperMikeII, the LSU supercomputer offers total 382 computing nodes, however, maximum of 128 can be allocated at a time. 
Each node has two 2.6GHz 8-core Sandy Bridge Xeon 64-bit processors, 32GB RAM and 500GB HDD. However, only 240GB local hard disk space is available as filesystem data storage for Hadoop. The rest is mounted on Lustre.
All nodes are connected through 40Gb/s infiniband network with 2:1 blocking ratio.
We also tested the large data set (eg. 450GB of human genome) in a 15-node Supermicro cluster with 256GB RAM, 12TB hard disk and 32 cores per node.
The high size of RAM, hard disk and higher number of cores per node are exactly in contrast with SuperMikeII.
We use it in order to pinpoint the problems and challenges that we observed in SuperMikeII (discussed in section 5b)
%Successful assemblies in this cluster motivates us to make use of NSF cloud resource that is tuned for current big data research.



% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation Results}
%\label{fig_sim}
%\end{figure}

% Note that IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command, the
% \label for the overall figure must come after \caption.
% \hfil must be used as a separator to get equal spacing.
% The subfigure.sty package works much the same way, except \subfigure is
% used instead of \subfloat.
%
%\begin{figure*}[!t]
%\centerline{\subfloat[Case I]\includegraphics[width=2.5in]{subfigcase1}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{subfigcase2}%
%\label{fig_second_case}}}
%\caption{Simulation results}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.


% An example of a floating table. Note that, for IEEE style tables, the 
% \caption command should come BEFORE the table. Table text will default to
% \footnotesize as IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that IEEE does not put floats in the very first column - or typically
% anywhere on the first page for that matter. Also, in-text middle ("here")
% positioning is not used. Most IEEE journals/conferences use top floats
% exclusively. Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the \fnbelowfloat
% command of the stfloats package.
\section {Result and Analysis}

\section {Related Work}
In the last few years Hadoop and the related projects in its ecosystem revolutionized our ability to find signals in noise.
These state of the art big data analytics softwares are actively used and studied in the last few years in the context of enterprise level data analytics job. 
On the other hand, they are hardly explored for high performance computing involving big data, although, their ability to handle terabyte scale data, support for a broad range of platform , flexible yet efficient programming model can make them potentially a good choice to efficiently address many of these challenges.
This works as one of the motivation of this work.
We broadly classified the existing bigdata analytics software into two different categories:
1) disk based processing and 
2) in-memory processing.
The availability of more disk space than memory is the main motivation for disk based processing.
Since, Hadoop was developed to analyze tera/petabyte scale data on commodity hardware it embraced a disk based processing.
But, reading data from disk is sevral magnitudes slower than reading from DRAM. 
The growing demand for faster processing entangled with iterative limitation of Hadoop motivated the community to embrace in-memory solution
According to the computation model offered by them we further classified them into three major categories 
1) Map-Reduce
2) Vertex centric Graph processing
3) Distributed Key-value Storage.
Apache Spark with their datastructure called resilient distributed dataset (RDD) is a unified framework that provides in-memory solution for all map-reduce, vertex centric graph processing and distributed key-value storage.
Giraph is a framework that provides an in-memory solution for synchronous vertex centric graph processing.
HBase is a disk based distributed key-value store aimed at application 
Our contribution is to show the performance benefit that a disk based application expect from SSD and a network intensive application can expect from different types of network architecture so that an HPC user can easily choose their intended architecture.

Appusswami et al.[] studied the problem of scaleup versus scaleout for enterprise level job that take less than 100GB of input.
Micheal studied the same problem for interactive query processing.
Our work is different from their in terms of data size and complexity of the algorithm 

\section{Conclusion}
The conclusion goes here.




% conference papers do not normally have an appendix


% use section* for acknowledgement
\section*{Acknowledgment}


The authors would like to thank...





% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
\begin{thebibliography}{1}

\bibitem{IEEEhowto:kopka}
H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
  0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.

\end{thebibliography}




% that's all folks
\end{document}


