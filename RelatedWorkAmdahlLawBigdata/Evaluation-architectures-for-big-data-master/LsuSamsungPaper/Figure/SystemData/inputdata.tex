High throughput next generation DNA sequencing machines like Illumina Genome Analyzer produce huge amount of short read sequences typically in the scale of several GigaBytes to Terabytes.
Furthermore, the size of the de Bruijn graph built from these vast amount of short reads may be another magnitude higher than the reads itself making the entire assembly pipe line severely data-intensive.
In this paper, we use the bumble bee genome and human genome sequence as a representative data sets.
The first one, bumble bee genome is available in Genome Assembly Gold-standard Evaluation (GAGE \cite{bio:gage}) website in fastq format.
The data size is 85GB containing almost 1billion reads.
The size of the de Bruijn graph produced by it is 90GB.
The second data set, the human genome is openly avalible in NCBI web site with accession no. SRX10639 in a compressed SRA format.
We decompress it using NCBI SRA toolkit which produce 452GB of short read sequence data in fastq format.
The size of the de Bruijn graph produced by it is almost 3.8TB which is almost 9 times higher than the reads itself.
The variation between the size of the input reads and the corresponding de Bruijn graph depends upon the sequencing-quality or pre-assembly-processing \cite{bio:quality1}, \cite{bio:quality2} that is out of the scope of this paper. 


%The size of the de bruijn graph varies severely with the quality of the sequencing experiments which 


%Fig. 1a shows the size of intermediate shuffled data and the actual size of the de Bruijn graph generated by the graph building phase of PGA for some of the data sets. For large scale genome assembly (eg. human genome) the shuffled data indicates very high local disk space requirment, whereas, the actual graph size indicates the huge memory requirement for in-memory graph analysis with Giraph in subsequent phases.
%In this paper we present the performance result  of different clusters with PGA for the human genome assembly.
%The data set is openly available in NCBI website with accession no. SRX10639 in a compressed SRA format.
%We decompress it  using NCBI SRA toolkit which produce 452GB of short read sequence data in fastq format which is stored in HDFS and serves as the input to PGA.
%In the first stage of PGA which is a Hdoop job to construct de Bruin graph, we create a 3.8TB of graph from these short read sequences which is anayzed in memory with Giraph in the succesive phases.
%Furthermore,the entire process of human genome assemby produces almost 15TB of temporary data that need to be stored in HDFS.
%In the next subsection we describe the resources that we used to accomodate and analyze this huge volume of data 