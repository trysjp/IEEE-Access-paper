%Recently, Hadoop and the rich software ecosystem that is built surrounding it became the defacto statndard of distributed computing involving bigdata.
%In the last few years, their performance is well explored for several data intensive enterprise application including log processing, machine learning, indexing etc.
%On the other hand, it is hardly explored for scientific application which involves terabyte scale data (eg. genome assembly) which are traditionally addressed by HPC, however, their flexible programming model can address many of them.
%Unlike the enterprise appliction these are highly tuned for existing supercomputers which differs from the scaledout clusters of commodity hardware in many aspect including CPU (in terms of FLOP), network (infiniband and ethernet) etc. However Hadoop initially targetted the commodity clusters, intersting it supports many of the supercomputing aspect also. 
%Not only they perform well over infifiniband, recent studies showed their io throughput can be incresed heavily using solid state drives (SSD) which is several magnitudes faster than normal hard disk drive (HDD).
%This flexible programming model offered by these bigdata analytics sofetware and their support for varying range of infrastructure creates a growing interest in terms of efficient provisioning of hardware which creates a gap between two otherwise convrging field, Bigdata Analytics and High Performance Computing

%In this  paper we discuss about ebout experimenting our parallel Hadoop and Giraph based genome assembler (PGA) that we developed to address the challenges involved in largesacle genome assembly, an HPC problem that recently made its way to the forefront of bigdata.
%We juxtapose current state of the art big data analysis software (Hadoop and Giraph) and available distributed computing resources with our assembler that serves as a very good example of both data as well as computation intensive job.
%We performed a series of experiments on different types of clusters including LSU supercomputers and Samsung clusters which vary in terms of storage (SSD and HDD), network (IB and Ethernet), processor density, available memory etc.
%we present a detail result and analysis of performance of PGA on these clusters in terms of io, cpu and memory utilization and networking. 
%We believe it will serve as one of the initial but solid foundation to answer 'how does the next generation high performance computation cluster to handle bigdata (tera/petabyte scale) should look like'

%In the last few years Hadoop and the rich software ecosystem that is built surrounding it became the de facto standard for the distriuted computing involving bigdata (tera/petabyte scale).
%The flexible programming model offered by the current state of the art bigdata analysis software (Hadoop, Giraph etc.), and their support for varying range of distributed cyber infrastructure including different types of storage (SSD and HDD) and network architecture (Infiniband and Ethernet) creates a growing interest in the research community to apply them for HPC challenges.
%Despite the growing interest, efficient provisioning and fine tuning these massively distributed system still remain a major challenge, especially in the context of high performance computing.
%On the other hand, most of the recent studies that analyze Hadoop's performance on differnet infrastructure, are limited to the enterprise data analytics jobs only (eg. log processing, sorting etc.),  that take less than 100GB of input [Microsoft][ChenFB], thereby ignored the hpc aspect of bigdata entirely both in terms of complex computation as well as huge in-memory processing.
%Although, several recent studies [Microsoft][IBROCEcomp][HiTune][SSDFlashKorea] analyze the performance of Hadoop on differnet types of clusters, these studies being limited to the enterprise data analytics job only that take less than 100GB of input [Microsoft][ChenFB], ignores the hpc aspect of bigdata entirely.
%Furthermore, they use the existing benchmarks (eg [hibench]) which are not made reflect the behavior of HPC problems which includes complex algorithms and highly tuned for the existing supercomputers.
%In this paper we discuss about experimenting PGA, a Hadoop and Giraph based Parallel Genome assembler that we developed to address the challenegs involved in large scale genome assembly, an HPC application that recently made its way to the forefront of bigdata.

Efficient processing of bigdata produced by different scientific experimental facilities poses several challenges in terms of efficient storage, transfer, in-memory computation and exploiting locality of these data. 
The existing supercomputers being optimally tuned for high performance compute intensive applications like MPI, Grid etc cannot address most of these challenges especially due to their storage and memory constraint.
On the other hand, in the last decade, Hadoop and other state of the art bigdata analytics softwares shifted the existing model of computation severely towards data driven analysis creating new oportunity to address many HPC challenges involving bigdata.
Although, these massively parallel bigdata analytics softwares made several HPC computation easy, efficient provisioning in terms hardwares still remain a major challenge especially in the context of data intensive high performance computation.

In this paper we juxtapose different distributed cyber infrastructure including traditional supercomputers and available cloud resources with Intel hiBench and our bigdata Parallel Genome Assembler (PGA) based upon Hadoop and Giraph, that serves as a very good example of data as well as compute intensive application.
We compare the perfromance of LSU supercomputer QueenBeeII and two different types of clusters located in Samsung Korea with our assembler and observed Samsung Cluster with [SSD and 10GigE....] outperformed QueenBeeII in terms of [price to performance....] because of [high processor density, higher IO throughput....].
%In this paper we juxtapose the current state of the art bigdata analytics softwares and the available distributed computing resources (supercomputers and other cloud resources) with our Parallel Genome Assembler based on Hadoop and Giraph called PGA, that we developed to address the challenges involved in large scale de novo genome assembly, a high performance compute intensive problem that recently made its way to the forefront of the bigdata challenges.
%PGA successfully assembled large scale mammalian genome including human genome of size 450GB that produces almost 4TB de Bruijn graph \cite{bio:debruijngraph} for analysis in different types of clusters with with different network and storage architecture. In this work we selected n different jobs from the entire human genome assembly pipeline representing the data intensive supercomputing workload and compare their performance in different types of clusters including LSU supercomputers and clusters located in Samsung, Korea. Our performance analysis shows that, Samsung cluster with SSD and 10GigE outperformed the supercomputers due to their higher IO throughput, substantial network speed, higher memory availability and high processor density. 
%Our work is motivated by the growing demand for developing comprehensive next generation distributed cyberinfrastructure in support of bigdata high-performance scientific and engineering applications. 
Unlike other studies, our analysis is not restricted in comparing any single system component (eg. storage or network interconnection) rather motivated to answer a higher level question that is becoming increasingly important 'how does the next generation bigdata (tera/petabyte scale) high performance computation cluster should look like'

%This paper is motivated by the growing demand for developing comprehensive next generation distributed cyberinfrastructure in support of bigdata high-performance scientific and engineering applications.
%The flexible programming model offered by the existing opensource state of the art bigdata software like Hadoop and the softwares in its ecosystem, and their support for different cyberinfrastructure both in terms of storage (SSD and HDD) and network (Ethernet and infiniband) made it possible to use them for HPC application which posed catalytic effect on this demand. %however, the research seems to be hindered by unavailability of proper benchmark or some examplary software.
%In this paper, we consider large scale genome assembly as an example HPC problem that recently made its way to the forefront of bigdata. 
%We juxtapose the current state of the art big data analytics software and the available distributed computing resources with our Parallel Hadoop and Giraph based bigdata Genome Assembler, PGA,  that serves as a very good example of both data as well as computation intensive job.

%We compare PGA's performance on different types of clusters and found that the Samsung cluster with SSD outperformed the supercomputers due to their higher IO throughput, higher memory availability and high processor density. %Also our cost analysis shows the . %storage (SSD and HDD) as well as network architecture (Infiniband and Ethernet).
%We believe, the performance results of PGA and our detailed analysis will serve as one of the initial however, solid foundation to answer the question that is becoming increasingly important 'how does the next generation high performance computation cluster to handle bigdata (tera/petabyte scale) should look like'
