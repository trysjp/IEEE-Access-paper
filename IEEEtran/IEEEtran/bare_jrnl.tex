

%% bare_jrnl.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% see http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



\documentclass[journal]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[journal]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.


\usepackage{paralist}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tablefootnote}



% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
   \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

%Commands
\newcommand\floor[1]{\lfloor#1\rfloor}
\newcommand\ceil[1]{\lceil#1\rceil}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Augmenting Amdahl's Second Law:  A Theoretical Model for Cost-Effective Balanced HPC Infrastructure for Data-Driven Science}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%

\author{
Arghya Kusum Das, %~\IEEEmembership{Member,~IEEE,}
Jaeki Hong, %~\IEEEmembership{Life~Fellow,~IEEE}
Sayan Goswami,
Richard Platania,\\
Kisung Lee, %~\IEEEmembership{Fellow,~OSA,}
Wooseok Chang,
Seung-Jong Park \\
Division of Computer Science and Electrical Engineering, Center for Computation and Technology, Louisiana State University, Baton Rouge, LA, USA, 70803\\
Samsung Electronics Co., Ltd., 95, Samsung 2-ro, Giheung-gu, Yongin-si, Gyeonggi-do, S. Korea, 446711
% <-this % stops a space
\thanks{A. K. Das, K. Lee, and S.J. Park are with the Department
of Computer Science and Electrical Engineering, Center for Computation and Technology Louisiana State University, Baton Rouge, USA}
\thanks{J. Hong and W. Chang are with Samsung Electronics, S. Korea}
%GA, 30332 USA e-mail: (see http://www.michaelshell.org/contact.html).}% <-this % stops a space
%\thanks{J. Doe and J. Doe are with Anonymous University.}% <-this % stops a space
%\thanks{Manuscript received April 19, 2005; revised August 26, 2015.}
}

% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2015}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.




% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\begin{abstract}
Recent compute technologies for analyzing enterprise and scientific big data have started demanding more processors, storage, and memory resources, forcing a similar growth in the total computation cost. Consequently, HPC system designers are facing relentless pressure to reduce the system cost while also providing expected performance for data- and compute-intensive applications.  With this motivation, this paper proposes a simple, easy to use, additive model to optimize cost/performance by quantifying the system balance among CPU, I/O  bandwidth, and size of DRAM in terms of both application characteristics and hardware cost. The proposed performance model theoretically augments Amdahl's  second law for a balanced system (Amdahl's I/O and memory number) and reveals that a balanced system needs almost 0.17GBPS I/O bandwidth, and almost 3GB of DRAM per GHz of CPU speed, considering Intel Xeon processing architecture and current price trend in processor, storage and memory.
%The I/O- and memory-bound nature of data-driven scientific applications are posing several challenges to next generation HPC system designers as they strive for system balance with respect to processor speed, I/O bandwidth and memory while also maintaining a balance between performance and economy.  Augmenting Amdahl's design principles for balanced system, collectively known as \textit{Amdahl's second law} in the context of current big data analytic software (e.g., Hadoop, Giraph, etc.) and recent advances in hardware is relevant to design cost efficient HPC infrastructure for data-intensive scientific applications. This paper proposes a simple, easy to use, general purpose, additive model for cost/performance to quantify and optimize the system balance (among CPU, I/O  bandwidth and size of DRAM) in terms of both software characteristics as well as the current hardware cost. Our performance model reveals that a balanced HPC system needs almost 0.19GBPS I/O bandwidth, and almost 3GB of DRAM per GHz of CPU speed considering Intel Xeon processing architecture which is widely used by the HPC system designers. However, our model can be used for any given architecture. To the best of our knowledge, this is the first attempt to theoretically extend Amdahl's second law considering more degrees of freedom, such as, workload characteristics and hardware cost.

To substantiate our claim, we evaluate three fundamentally different cluster architectures, 1) a traditional HPC cluster called SupermikeII, 2) a regular datacenter called SwatIII, and 3) a novel MicroBrick based hyperscale system called CeresII. CeresII has 6-Xeon cores each running at 2GHz, 1-NVMe SSD with 2GBPS I/O bandwidth and 64GB DRAM, which closely resembling the optimum produced by our model. We evaluated these three clusters with respect to two widely used Hadoop-benchmarks (TeraSort and WordCount) as well as our own genome assembler based on Hadoop and Giraph, which serves as a real world example of a data- and compute-intensive workload. CeresII outperformed all other cluster architectures for all the benchmarks in terms of both execution time and cost/performance. For a large human genome assembly, CeresII showed more than 85\% improvement over SuperMikeII and almost 40\% improvement over  SwatIII in terms of cost/performance.   
\end{abstract}

% Note that keywords are not normally used for peerreviewl
\begin{IEEEkeywords}
Cost to Performance, Balanced System, Hadoop, Giraph.
\end{IEEEkeywords}

% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction}
% The very first letter is a 2 line inial drop letter followed
% by the rest of the first word in caps.
% 
% form to use if the first word consists of a single letter:
% \IEEEPARstart{A}{demo} file is ....
% 
% form to use if you need the single drop letter followed by
% normal text (unknown if ever used by the IEEE):
% \IEEEPARstart{A}{}demo file is ....
% 
% Some journals put the first two words in caps:
% \IEEEPARstart{T}{his demo} file is ....
% 
% Here we have the typical use of a "T" for an initial drop letter
% and "HIS" in caps to complete the first word.
%%\IEEEPARstart{T}{his} demo file is intended to serve as a ``starter file''
%%for IEEE journal papers produced under \LaTeX\ using
%%IEEEtran.cls version 1.8b and later.
% You must have at least 2 lines in the paragraph with the drop letter
% (should never be an issue)
%%I wish you the best of success.

%%\hfill mds
 
%%\hfill August 26, 2015

\IEEEPARstart{C}{urrent} compute technologies for enterprise and scientific big data analysis are demanding more compute cycles per processor, with extreme I/O performance also required.  At the same time, the hardware cost for storing and processing this big data is increasing linearly with data volume. Consequently, today's system designers are facing relentless pressure to reduce the system cost while also providing the expected level of performance. Complicating the scenario, the exponential growth of big data is rapidly changing the fundamental model of computation involved in high performance computing. A growing number of codes in both enterprise and scientific domain are being written using state of the art big data analytic software, such as Hadoop and Giraph which carefully consider data locality. 

%Traditional supercomputers focusing oly on doing calculation at blazing speed cannot produce expected level of performance. 
%A systematic and general approach is needed for these data-intensive applications both in terms of a computational model as well as cost-effective and balanced cyber infrastructure. The best practical approach till date was originally proposed by Jim Gray in 2000 (known as Gray’s Laws [2] [3]) where he stated, 1) Bring computations to the data, rather than data to the computations, and 2) The solution is in a scale-out architecture. With the increasing data size, the first law, which basically defines the locality-based model of computation utilizing lower network bandwidth, seems to govern the scientific big data analysis for next several years in future. For example, scientists are increasingly using Hadoop and other state of the art big data analytic software which carefully consider data locality. On the contrary, the second law, which defines the underlying distributed cyber infrastructure, needs to be clarified. 

As a consequence, most of the existing high performance  computing (HPC) clusters focusing only on tera to peta FLOP scale processing speed are found to be unbalanced for data-intensive scientific applications either in terms of performance, or economy, or both (\cite{Amdahl:Workloadchang}, \cite{cluster:AmdahlBalancedBlade}). The existing data center architectures also need to be reexamined. Although the current state of the art big data analytic software were initially developed to work well on scale-out clusters of cheap commodity hardware, recently published results showed that scale-up can also provide better result for different data-intensive applications both in terms of performance and cost (\cite{scaleupscaleout:appuswamy}) . 

As a matter of fact, system characteristics for data-intensive applications vary tremendously, especially because of varying nature and volume of data, leading to no single solution in terms of underlying cluster architecture. The problem becomes more complicated in a public cloud environment or an HPC cluster where the application characteristics are not familiar beforehand. In these scenarios, when the application characteristics are unfamiliar, it makes sense to invest in a balanced cluster as an initial approach. Hardware vendors, as well as cloud vendors (e.g., Amazon, Google, R-HPC, etc.) are investing a huge amount of money towards this. Millions of dollars are being spent for programs such as NSF Cloud\footnote{https://www.chameleoncloud.org/nsf-cloud-workshop/} or XSEDE\footnote{https://www.chameleoncloud.org/nsf-cloud-workshop/}, where several industries and universities collaborate to find such balanced architectures to drive next generation cloud research. 

At this inflection point of HPC landscape, system designers must consider more degrees of freedom for new cluster architecture for big data than for existing HPC clusters which focus only on doing calculation at blazing speed. They must address such questions as: \textit{how much I/O bandwidth is required per processing core?} \textit{How much memory is required to optimize performance and cost?}  Similar questions can be asked for scale-up and scale-out architectures also, as there is no concrete definition available for these. These complex performance and economic factors together motivate a new design of HPC infrastructure. However, they pose several challenges to the system designers who are striving for system balance in terms of processing speed, I/O bandwidth, memory, and the cost of the infrastructure. 

To summarize, as the scientific research is becoming more data-driven in nature, it is obvious that providing more resources to an HPC cluster (processing speed, I/O bandwidth, DRAM)  will obviously improve the performance of data-intensive scientific applications. But at what cost? So, the major challenge to the system designers nowadays is not in providing only high performance but in providing expected performance in reduced cost. There is limited analytical studies that address the challenges in developing a cost-effective, balanced distributed-cyber-infrastructure to analyze large-scale scientific big data.
 
With this motivation, this paper makes an initial attempt to resolve the existing performance and cost conundrum by theoretically augmenting Amdahl's second law for balanced system. This paper proposes a simple additive model to optimize cost/performance by quantifying the system balance between CPU speed, I/O bandwidth, and size of DRAM in terms of software application characteristics and current trend in hardware cost. Our model does not assume any specific software framework or hardware technologies, rather, it provides an easy to use, general guideline for designing a balanced system that can provide good performance in reduced cost. However, the outcome of the model (i.e., configuration of an optimal balanced system) suggests to use solid state drives (SSD) instead of hard disk drive (HDD) in a multicore machine to maintain the system balance. Assuming an equal distribution of I/O  and compute work in a data-intensive application, our model suggests that a balanced HPC system needs almost 0.19-GBPS I/O bandwidth, and almost 3-GB of DRAM per GHz of CPU speed using Intel Xeon processor  and current price trend of different hardware. 

%We observed a convergence between our cost model and Amdahl's design principle. It is to be noted, we did not confine ourselves in proposing a performance model for any specific big data analytic software (e.g., Hadoop). Rather, we proposed a theoretical performance model for the underlying hardware only which is required by these big data analytic software for optimal performance. Thus, we are motivated to answer a more general question \textit{How does a next generation balanced HPC cluster should look like}

To substantiate our claim, we evaluate three fundamentally different cluster architectures as follows: 1) a traditional HPC cluster called SuperMikeII (located at LSU, USA), that offers 382 computing nodes, each with 16Xeon cores, 1HDD, and 32GB DRAM, 2) a regular datacenter architecture called SwatIII (located at Samsung, Korea), that has 128 nodes, each with 16Xeon cores, 4HDD, and 256GB DRAM and 3) a new MicroBrick-based architecture called CeresII (also located at Samsung, Korea). Each CeresII node has  6Xeon cores, 1NVMe SSD  with 2GBPS I/O bandwidth and 6GB DRAM per node, thus closely resembling the optimum produced by our model. We evaluated these three architectures with respect to three different benchmark applications. Two of them are widely  used benchmark Hadoop applications (TeraSort and WordCount), and the other one is our own benchmark genome assembler developed atop Hadoop and Giraph, which serves as a good real-world example of a data-, compute- and memory-intensive workload. For all three benchmark evaluations, CeresII with the most optimal configuration, outperformed the others in terms of both performance and cost/performance. For a large human genome assembly, CeresII showed more than 85\% improvement over SuperMikeII and almost 40\% improvement over SwatIII in terms od cost/performance%when using same amount of processing cores as SuperMikeII and SwatIII across the cluster, and significantly less (almost half) DRAM than SwatIII. 

The rest of the paper is organized as follows: Section-\ref{sec:RelatedWork} describes the prior work relate to our current effort. In Section-\ref{sec:AmdahlGray}, we discuss Amdahl's second law in detail. Section-\ref{sec:Model} describes the proposed model. Then, in Secton-\ref{sec:ExperimentalTestbed} we show the details of our experimental testbeds and classify those using our proposed model. Section-\ref{sec:EvalMethod} describes the evaluation methodology for these clusters (i.e., the details of the software and benchmark that we used in our evaluation). In Section-\ref{sec:Result} we discuss the experimental results. Section-\ref{sec:Limitation} discuses the limitations in the current model to stimulate discussion and future work. Finally, Section-\ref{sec:Conclusion} concludes the paper.

\section{Related Work} \label{sec:RelatedWork}
Numerous studies have been performed evaluating the performance implication of different big data analytic software on different types of hardware infrastructures. We categorize these existing studies into four different classes: 1) experimental evaluation of different cluster architectures for big data software, 2) simulation of big data analytic software, 3) analytical performance modeling of big data analytic software, and 4) system characterization using Amdahl's second law, that is the most relevant to our current effort. %In this section we discuss the previous works done in these three categories.

\subsection{Experimental evaluation of different cluster architecture for big data software.}  %Kang \cite{ssdhdd:kang} compared the execution time of sort, join, WordCount, Bayesian, and DFSIO workloads using SSD and HDD and obtained better performance using SSD. Wu \cite{ssdhdd:wu} found that Hadoop performance can be increased almost linearly with the increasing fraction of SSDs in the storage system. They used the TeraSort benchmark for their study. Additionally, they also showed that in an SSD-dominant cluster, Hadoop's performance is almost insensitive to different Hadoop performance parameters such as block-size and buffer-size. Moon \cite{ssdhdd:moon} showed a significant cost benefit by storing the intermediate Hadoop data in SSD, leaving HDDs to store Hadoop Distributed File System (HDFS \cite{fw:hdfs}) data. They also used the TeraSort benchmark in their study. A similar result can be found in the study by Li \cite{ssdhdd:li} and Krish \cite{ssdhdd:krish} where SSDs are used to store temporary data to reduce disk contention and HDDs are used to store the HDFS data. They all reached the same conclusion as Moon \cite{ssdhdd:moon}.  Tan \cite{ssdhdd:tan} also reached the similar conclusion for two other workloads including a Hive workload and an HBase workload. 

Michael \cite{scaleupscaleout:michael} investigated the performance characteristics of scale-out and scale-up architectures for interactive queries and found better performance using a scale-out cluster than a scale-up. On the other hand, Appuswamy \cite{scaleupscaleout:appuswamy} reached an entirely different conclusion in their study. They observed a single scale-up server to perform better than an 8-node scale-out cluster for eleven different enterprise-level Hadoop applications, including log-processing, sorting, and Mahout machine learning. Several studies have also been performed evaluating the impact of different storage media (SSD and HDD) or network configuration on Hadoop. For example, Kang \cite{ssdhdd:kang}  and Wu \cite{ssdhdd:wu} argued that SSD improved the performance of Hadoop clusters with repect to different Hadoop benchmarks such as WordCount, DFSIO, TeraSort, etc. Moon \cite{ssdhdd:moon} showed a significant cost benefit in TeraSort by storing the intermediate Hadoop data in SSD, leaving HDDs to store Hadoop Distributed File System (HDFS \cite{fw:hdfs}) data. A similar result can be found in the study by Li \cite{ssdhdd:li} and Krish \cite{ssdhdd:krish}, where SSDs are used to store temporary data to reduce disk contention, and HDDs are used to store the HDFS data. 

In our previous study \cite{scaleupscaleout:das2015evaluating}, we aggregated many of these efforts together and reported the performance result for individual hardware components (network, storage, and memory) as well as their overall organization. Considering 9 different cluster configurations, we provided significant insight on  how to deploy SSDs better in Hadoop cluster. We also showed the imbalance between performance and economy that exists in scale-up and scale-out architectures. Although we experimentally showed that this imbalance was eliminated in a Samsung MicroBrick-based prototype cluster, we did not provide any quantitative analysis, for scale-up/out or for balanced system that can improve performance while staying within the budget. In fact, none of the prior efforts mentioned in this section provide such quantitative analysis which is extremely important to propose next-generation high-performance, balanced, distributed-cyber-infrastructures for data- and compute-intensive applications.

%However, an older study by Kung \cite{Balance:kung1986memory} can shed light on feasibility of the scaling up approach. Kung analyzed several HPC problems and theoretically showed that the memory needs to be increased exponentially to restore the balance of the system when the number of processors is increased by a certain factor. For example, best known external sorting algorithm needs the local memory to be increased by an exponent of $\alpha$ when number of processors are increased by a factor of  $\alpha$ (i.e., $M_{new} = M_{old}^{\alpha}$) to restore the balance of the system. Although this study is not directly related to this paper, some of the problems considered in \cite{Balance:kung1986memory} constitutes the core part of today's big data analytic framework. For example, Hadoop uses the similar external sorting in its shuffle phase. Hence, from the perspective of balanced system design arbitrary scaling up the size of DRAM is possibly not a feasible option. Rather, the amount of RAM should be decided based upon cost.

\subsection{Simulation of big data software}
Simulation is widely used for predicting performance of different big data software (mainly Hadoop) and analyzing design trade-offs in different hardware. 

At the border level, all simulators generate virtual Hadoop MapReduce jobs and tune different hardware and software parameters in a simulated environment, mostly using prior experiences or trial-and-error to optimize the performance of some parts of the Hadoop framework or a given Hadoop job. For example, HSim \cite{Simulator:liu2013hsim} focuses on the Hadoop job parameters to optimize the performance of the entire job. SimMR \cite{Simulator:verma2011play}, on the other hand, focuses on simulating the Hadoop task scheduling algorithms. MRSim\cite{Simulator:hammoud2010mrsim} and MRPerf \cite{Simulator:wang2009simulation} unified all these aspects in a single simulator. They simulate the hardware environment using discrete event simulator packages such as,  SimJava, GridSim, NS2, etc. Then  they execute the fake Hadoop job on a small subset of data to predict the performance. Given prior experience, simulators can predict architecture alternatives, thus enabling huge cost and effort savings comparing to experimental evaluation on real hardware discussed before. 

However, most of these simulators come with lots of overhead. They are time and resource consuming and often fail to assess the real system characteristics in the presence of huge volumes of big data. Furthermore, the trial-and-error method of performance optimization, considering the broad range of available hardware alternatives with more than $200$ Hadoop parameters, is challenging and most of the time unreliable.

\subsection{Analytical performance modeling of big data software}
Analytical models abstract away several performance parameters and predict the performance of a Hadoop job mainly using single- or multi-layer queuing networks. For example, Viana \cite{Model:vianna2013analytical} model the pipeline parallelism of a Hadoop job using a queuing network to predict the performance of the job.  Wu \cite{Model:wu2015exploring} proposed a layered queue network model to predict the performance of a Hadoop job in a cloud environment. In a recent work, Ahn \cite{Model:ahn2015analytical} proposed a queuing theory model to predict the performance of Hadoop job atop different storage technologies (i.e, HDD and SSD). Krevat \cite{Model:krevat2010applying} computed I/O complexity of Hadoop jobs for data-intensive applications and proposed a model to quantify the hardware resource wasted by Hadoop. 

Unlike simulation, there is no overhead involved in an analytical model approach. However, the available  alternatives for individual hardware components are huge in number. It is hard to find an optimally balanced architecture in this vast range of alternatives. Thus,  both simulation and analytical approaches that model the performance of big data software may work well in selecting alternatives between two or more existing hardware infrastructures (in a small finite search space), however, their capability is limited in proposing a new architecture.

\subsection{System characterization using Amdahl's second law }
To date, the most practical approach to design a balanced system is to follow Amdahl's second law.  Computer scientist Gene Amdahl  postulated several design principles for a balanced system, collectively known as Amdahl's Second Law.  He stated a balanced system needs 1bit of sequential I/O per second (Amdahl's I/O number) and 1byte of memory (Amdahl's memory number) per CPU instruction per second. These design principles for balanced system still hold true, even after 50 years with some ammendments proposed by Jim Gray in early 2000 \cite{Amdahl:RuleofThumbgray2000rules}. Gray observed that Amdahl's I/O number still holds true but should be measured with respect to the relevant workload. He also observed that  Amdahl's memory number is rising from 1 to 4. Gray's amendments are based on contemporary observation and are not formalized. We will discuss both Amdahl's second law and Gray's amendment in more details as well as formalize Gray's amendment later in Section-\ref{sec:AmdahlGray}. 
%Based upon these studies, Dobos and Szalay \cite{Amdahl:Graywulfdobos2013graywulf} proposed a balanced system called GrayWulf for data intensive computing that won the storage challenge in SuperComputing 2008. 

Amdahl's second law (with Gray's ammendment) can be used for system characterization and proposing a balanced system. For example, Bell and Gray \cite{Amdahl:PetascaleBell2005petascale} classified the existing supercomputers based upon Amdahl's second law to clarify the future road map of the HPC architecture. Cohen \cite{Balance:cohen2009applying} applied Amdahl's second law to the datacenter cluster to study the interplay between processor architecture and network interconnect in a datacenter. Chang  \cite{Amdahl:Workloadchang} used Amdahl's second law to better understand the design implications of data analytic systems by quantifying workload requirements. Szalay \cite{cluster:AmdahlBalancedBlade}, using Amdahl's second law, proposed a new cluster architecture  based on SSD and low power processors (such as, Intel Atom, Zotac etc) to achieve a balance between performance and energy efficiency. In this study, the authors ignored the CPU micro architecture and simplified the Amdahl's I/O and Memory number by dividing the I/O bandwidth (bit/s) and size of DRAM (GB) by CPU-speed (GHz) instead of using instructions per second (IPS). The cluster architecture was balanced as the simplified Amdahl's I/O number was close to 1bit/s/IPS. In a recent study \cite{Balance:zheng2014hadoop} Zheng evaluated a similar architecture as Amdahl's balanced blade for Hadoop applications and showed Atom processor is the system's bottleneck.  %In a recent work, Liang [25] applied Amdahl’s second law for performance characterization of two different state of the art big data analytic software (e.g., Hadoop and DataMPI).  

Unlike Szalay, we consider a balanced system as one that optimizes both cost and performance. Hence, we did not consider the optimal balance of the system (i.e., the system's I/O and memory ratio to the processor speed) as constants as in Amdahl's I/O and memory number. Instead, we express the optimal system balance  as a function of both application characteristics and hardware price (partly inspired by \footnote{www.cs.virginia.edu/~mccalpin/SPEC\_Balance\_2007-06-20.pdf}). 
%\section{Motivation} \label{sec:Motivation}
%There are several important reasons to balance the The best practical approach till date was originally proposed by Jim Gray in 2000 (known as Gray’s Laws ) where he stated, 1) Bring computations to the data, rather than data to the computations, and 2) The solution is in a scale-out architecture. With the increasing data size, the first law, which basically defines the model of computation utilizing lower network bandwidth, seems to govern the field of big data HPC for next several years in future. For example, scientists are increasingly using Hadoop and other state of the art big data analytic software which carefully consider data locality. However, the second law, which defines the underlying distributed cyber infrastructure needs to be quantified.  

%\subsection{Storage Architecture}
%Traditional HPC clusters are normally equipped with only one hard disk drive per compute node, thus show extremely low I/O bandwidth. Over the span of last 10 years the the hard disk speed is improved by only twice from 7000RPM to 15000RPM. Consequently, the I/O throughput increased from 80MB/s to 150MB/s. At this rate, to read just a 500GB hard disk takes almost one hour. In real world scenario, where a huge amount of data is read/write from/to the local disk the performance may be even worse. So, in many state of the art data center cluster and cloud infrastructure the data is stripped across multiple smaller disks to improve the I/O throughput.

%Alternatively, the SSDs can be used. It uses the similar flash memory that is used in memory subsystem, thus increases the per-disk I/O throughput by almost 4-times than an HDD. Current SSDs offer almost 550MB/s I/O throughput in a moderate cost while maintaining considerable storage capacity. Due to the high bandwidth of SSDs, the most intuitive approach to build a high performance cluster is to use multiple SSDs with high end processors. However, the disk controllers are found to

%\subsection{Memory Architecture}
%With the introduction of DDR (Double Data Rate) technology the memory bandwidth has been improved significantly because it reads one word of data during the positive edge and one word during the negative edge of the processor clock pulse. Most of the current HPC cluster as well as the computation clusters use either DDR2 or DDR3 RAM (random access memory or, simply main memory) modules. There is hardly any difference among current clusters (including traditional HPC clusters and datacenters) in terms of memory architecture or processormemory communication model. 

%However, for improved performance, suffcient memory should be provided to the cluster that can hold the required computation in conjunction with the big data. Furthermore, more memory improves the caching effect which again improves the applications’ performance. However, the main memory is costly which makes the job of building the balanced system complicated. Again, the SSD is nowadays increasingly considered as a new layer between memory (Since they use similar flash arrays as RAM) and storage due to its increased I/O bandwidth which is changing the designer’s approcah towards a balanced system. 

%\subsection{Network Architecture}
%Traditional HPC clusters uses a hierarchy of switches with a fat tree model of connectivity. This approach typically uses layered architecture. The hosts or servers are connected to the bottom layer, called access layer which is then aggregated to an intermidiate layer of switches, called edge layer or leaf. The edge layer switches are then connected to the top layer switches, called core layer or spine where the actual bandwidth of the network is scaled. To prevent over subscription, the link speeds got progressively higher from access layer to leaves to spine starting from only few Mbps to several Gbps. This architecture may suffer from bottleneck issues thereby introduces architectural imbalance in the modern datacenter or cloud infrastructures as the bandwidth of the host adapter is increasing in an outstanding pace (an observation by Cohen [24]). Furthermore, to accomodate many servers with fewer switches (thus, reducing the cost of the network) a blocking is used which again limits the performance of big data genomic applications which demand more bandwidth.

%As an alternative, the simple Clos based architecture is gaining popularity where all the lower layer switches (i.e., leaf) are connected to all the top layer switches (i.e., spine) using a full mesh topology, thus achieving a non-blocking model using inexpensive devices. The data-intensive applications based of Hadoop or Giraph, which transmits huge amount of data over network in different phases of computation, can use more bandwidth from this all-to-all connection model. Although we did not include network 

\section{Amdahl's Second law, Gray's Ammendment, and Their Limitations} \label{sec:AmdahlGray}
\subsection{Original Form of Amdahl's Second Law}
Computer scientist Gene Amdahl postulated several design principles in the late 1960 for a balanced system. As mentioned earlier, these design principals are collectively known as Amdahl's second law,  which are as follows:
\begin{enumerate}
\item \textit{Amdahl's I/O law:} A balanced computer system needs one bit of sequential I/O per second per instruction per second. From this point we will mention this law as Amdahl's I/O number. Alternatively, Amdahl's I/O number of a balanced system can be expressed as $0.125$ GBPS/GIPS (by changing in conventional units).
%\begin{equation} \label{eqn:AmdahlIoNo}
%Amdahl's I/O Number = \frac{I/O Bandwidth (bit/s)}{CPU Speed (GIPS)} = 1
%\end{equation}
\item \textit{Amdahl's memory law:} A balanced computer system needs one byte of memory per instruction per second. From this point, we will mention this law as Amdahl's memory number.
%\begin{equation} \label{eqn:AmdahlMemNo}
%Amdahl's Memory Number = \frac{Memory Size (GB)}{CPU Speed (GIPS)} = 1
%\end{equation}
\end{enumerate}
Using the notations in Table-\ref{tab:Notation}, Amdahl's I/O and memory numbers can be expressed as:
\begin{equation} \label{eqn:AmdahlIONotation}
\begin{split}
\beta_{io} ^{opt}= 0.125
\end{split}
\end{equation}
\begin{equation} \label{eqn:AmdahlMemNotation}
\begin{split}
\beta_{mem}^{opt} = 1
\end{split}
\end{equation}

%Amdahl's second law guided the designers for several decades to make balanced system. However, one should remember that this is an ideal. In practice, there are two major limitations as follows:
%\begin{enumerate} \itemsep0pt \parskip0pt \parsep0pt
%\item It overlooks the workload characteristics, i.e.,  the total amount of work done separately by CPU, I/O, and memory subsystem. It is important because of the diverse resource requirements, one-size-fit-all designs typically cannot satisfy the different resource balance ratios for a collection of analytic workloads. For example, using SSD for compute-intensive application may result in wastage of I/O bandwidth, thus may yield worse cost/performance ratio. %the traditional HPC workload performs smaller number of I/O.  That drives the designers to keep the Amdahl's I/O number of the existing HPC system in the range between $10^{-5}$ to $10^{-2}$ which is a magnitude smaller than the ideal value of $1$. 
%\item The law also ignores the cost of CPU, disk and DRAM. In this data-centric era, the main challenge to the HPC system designers is not providing only better performance but to maintain a balance between system's performance and cost. For example, increasing the size of DRAM may result in better performance most of the time because more DRAM can hold more data.  But, it may result in significantly worse cost/performance. In our previous work \cite{} we observed similar issue for genome assembly application.
%\end{enumerate} 
\subsection{Gray's Amendment to Amdahl's Second law}
Computer scientist Jim Gray reevaluated and amended Amdahl's second law in the context of modern data engineering. These amendments are collectively known as Gray's law. The revised laws are as follows:
\begin{enumerate}
\item \textit{Gray's I/O law:} A system needs 8 MIPS/MBPS I/O (same as Amdahl's I/O number, but in different unit), but the instruction rate and IO rate must be measured on the relevant workload.
\item \textit{Gray's memory law:} The MB/MIPS (alternatively, GB/GIPS) ratio is rising from 1 to 4. This trend will likely continue.
\end{enumerate}

The underlying implication of Gray's I/O Law is that it aims for systems whose Amdahl's I/O number  matches the Amdahl's I/O  numbers of the applications (i.e., application balance) that run on that system. In the memory law, Gray put forward statistics reflecting the contemporary state of the cluster architecture. 

Using the notations in Table-\ref{tab:Notation} Gray's laws can be expressed as follows:
\begin{equation} \label{eqn:GrayIONotation}
\begin{split}
\beta_{io}^{opt} = \gamma_{io}
\end{split}
\end{equation}
\begin{equation} \label{eqn:GrayMemNotation}
\begin{split}
\beta_{mem}^{opt} = 4
\end{split}
\end{equation}

%\begin{enumerate}
%\item It is becoming increasingly difficult and time consuming to look for ideal benchmark for modern data-intensive scientific applications which show tremendously varying system characteristics for varying nature and size of data. Some form of formalization is needed to address this issue so that system designers can at least estimate the workload characteristics, to be able to predict the hardware configuration. 
%\item Furthermore, the cost component is still ignored even in this revised version also.  The change in hardware price has already started changing the performance point and likely to continue in the future with change in technologies.
%\end{enumerate}   
\subsection{Limitations of Existing Laws}
Amdahl's second law for balanced systems does not consider the impact of application balance (or applications' resource requirement). Because of the diverse resource requirements, a one-size-fit-all design as suggested in the original law (expressed as the constants in the right hand side of Equation-\ref{eqn:AmdahlIONotation} and \ref{eqn:AmdahlMemNotation}), cannot satisfy the different resource balance ratios for a collection of analytic applications. 

Gray's law is more realistic in the sense that it consider the impact of application balance on the cluster architecture.  However, it is limiting to reflect the interplay between application balance and cost balance. The cost of hardware components has already changed the performance point and will keep on changing it as the technology continues to advance. 

\section{Proposed Model for System Balance} \label{sec:Model}
\begin{figure*}[htb]
	\begin{subfigure}[b]{0.48\textwidth}
                \includegraphics[width=\textwidth]{Figures/ModelFigures/beta_io.pdf}
                \caption{I/O balance}
                \label{fig:beta_io}
    \end{subfigure}
 	\begin{subfigure}[b]{0.48\textwidth}
                \includegraphics[width=\textwidth]{Figures/ModelFigures/beta_mem.pdf}
                \caption{Memory balance}
                \label{fig:beta_mem}
   \end{subfigure}
   \caption{Change in system's optimum I/O ($\beta_{io}^{opt}$) and memory ($\beta_{mem}^{opt}$) balance as a function of application balance ($\gamma_{io}^{opt}$ and $\gamma_{mem}^{opt}$) for different cost balance ($\delta_{io}^{opt}$ and $\delta_{mem}^{opt}$)}
  \label{fig:BetaVsGamma}
\end{figure*}
\subsection{Problem Definition}
Using the notations described in Table-\ref{tab:Notation}, the optimal system balance (i.e., $\beta_{io}^{opt}$ and $\beta_{mem}^{opt}$) needs to be expressed as a function of application balance (i.e., $\gamma_{io}$ and $\gamma_{mem}$) and cost balance (i.e., $\delta_{io}$ and $\delta_{mem}$). Mathematically it can be written as:

\begin{equation} \label{eqn:BalancedIONotation}
\begin{split}
\beta_{io}^{opt} = f_1(\gamma_{io}, \delta_{io})
\end{split}
\end{equation}
\begin{equation} \label{eqn:BalancedMemNotation}
\begin{split}
\beta_{mem}^{opt} = f_2(\gamma_{mem}, \delta_{mem})
\end{split}
\end{equation}

\begin{table}[!t]
\caption{Notations used in the model and their meaning}
\label{tab:Notation}
\centering
\begin{tabular}{|c|p{7cm}|}
\hline
$R_{cpu}$ & CPU speed of a given system $S$ (GHz)\\
$R_{io}$ &  I/O bandwidth of system $S$ (GBPS)\\
$R_{mem}$ & DRAM size of system $S$ (GB)\\
\hline
%$W$ & Suggested workload in Gray's ammendment for which Amdahl's second law holds\\
$W_{cpu}$ & Fraction of work done by the CPU for a given application $W$\\
$W_{io}$ & Fraction of work done by the disk(s) for $W$\\
$W_{mem}$ & Fraction of work done by DRAM for $W$\\
\hline
$P_{cpu}$ & Price per GHz of CPU speed\\
$P_{io}$ & Price per GBPS of I/O bandwidth\\
$P_{mem}$ & Price per GB of DRAM\\
\hline
\hline
$\beta_{io}$ & System balance between I/O bandwidth and CPU speed for system $S$ ($= R_{io}/R{cpu}$)\\
$\beta_{mem}$ & System balance between DRAM size and CPU speed for system $S$ ($= R_{mem}/R{cpu}$)\\
\hline
$\gamma_{io}$ & Application balance between CPU and I/O bandwidth for application $W$ ($= W_{io}/W{cpu}$). This term quantifies to what extent the application is I/O-intensive or CPU-intensive. Lower value means more CPU-intensive, higher value means I/O-intensive \\
$\gamma_{mem}$ & Application balance between CPU and DRAM size for application $W$ ($= W_{mem}/W{cpu}$). This term quantifies to what extent the application is memory-intensive or CPU-intensive. Lower value means more CPU-intensive, higher value means memory-intensive \\
\hline
$\delta_{io}$ & Cost balance between CPU and I/O bandwidth for system $S$ ($= P_{io}/P{cpu}$)\\
$\delta_{mem}$ & Cost balance between CPU and DRAM for system $S$ ($= P_{mem}/P{cpu}$)\\
\hline
\hline
$\beta_{io}^{opt}$ & Optimal system balance between I/O bandwidth and CPU speed (The problem under consideration)\\
$\beta_{mem}^{opt}$ & Optimal system balance between DRAM size and CPU speed (The problem under consideration)\\
\hline
\end{tabular}
\end{table}



%Let us consider $W$ is the relevant workload suggested in Gray's ammendment. if $W_{cpu}$, $W_{io}$, and $W_{mem}$ presents the amount of work done by CPU, I/O, and memory subsystem then  Gray's law can be written using the notation used in Table-\ref{tab:Notation} as follows,
%\begin{equation} \label{eqn:GrayLawNotation}
%\begin{split}
%\gamma_{io} = 0.125\\ 
%\gamma_{mem} = 1
%\end{split}
%\end{equation}
\subsection{Model Assumptions}
For simplicity of calculation and better usability, the model first ignores the CPU micro architecture. That is, we  consider the number of instruction executed per cycle (IPC) as proportional to CPU core frequency. Hence, express the balance between I/O and CPU in terms of GBPS/GHz, and balance between DRAM size and CPU in terms of GB/GHz. The practical implication of this assumption is that, the system designers may need to use this model repeatedly for different micro architectures based upon their corresponding IPC. It is feasible as micro architecture does not change as frequently as number of cores increases.
  
Second, for simplicity, we assume that the model is additive. That is, we ignore the overlap between work done by CPU, I/O, and memory subsystem. This way, the total execution time ($T_{total}$) of an application can be written as:
\begin{equation} %\label{eqn:TotalTime}
\begin{split}
T_{total} = T_{cpu} + T_{io} + T_{mem} \\
\end{split}
\end{equation}
\begin{equation} \label{eqn:TotalTime}
\begin{split}
\implies T_{total}  = \frac{W_{cpu}}{R_{cpu}} + \frac{W_{io}}{R_{io}} + \frac{W_{mem}}{R_{mem}}
\end{split}
\end{equation}
Third, we assume the total cost of the system as the summation of individual cost of CPU, I/O, and memory subsystems only. We ignore several constant components such as base cost, service cost, etc. This way, the total system cost ($C_{total}$) can be written as:
\begin{equation} %\label{eqn:TotalCost}
\begin{split}
C_{total} = C_{cpu} + C_{io} + C_{mem} \\
\end{split}
\end{equation}
\begin{equation} \label{eqn:TotalCost}
\begin{split}
\implies C_{total} = P_{cpu}R_{cpu} + P_{io}R_{io} + P_{mem}R_{mems}
\end{split}
\end{equation}
\subsection{Model Derivation}
Assuming the performance as the inverse of the total execution time, the cost/performance (denoted as $f_{cp}$) can be expressed as:
\begin{equation} %\label{eqn:CostPerPerformance}
\begin{split}
f_{cp} = C_{total} \times T_{total} \\
\end{split}
\end{equation}
\begin{equation} %\label{eqn:CostPerPerformance}
\begin{split}
\implies f _{cp}= (C_{cpu} + C_{io} + C_{mem}) \times\\ 
(T_{cpu} + T_{io} + T_{mem}) 
\end{split}
\end{equation}
\begin{equation} \label{eqn:CostPerPerformance}
\begin{split}
\implies f_{cp} = C_{cpu}T_{cpu} + C_{cpu}T_{io} + C_{cpu}T_{mem} \\
   + C_{io}T_{cpu} + C_{io}T_{io} + C_{io}T_{mem} \\
   + C_{mem}T_{cpu} + C_{mem}T_{io} + C_{mem}T_{mem} 
\end{split}
\end{equation}
By expanding all the time ($T$) and cost ($C$) terms using Equation-\ref{eqn:TotalTime} and \ref{eqn:TotalCost} respectively and then substituting with the notation used for system balance in Table-\ref{tab:Notation} (i.e., $\beta_{io}$ and $\beta_{mem}$),  Equation-\ref{eqn:CostPerPerformance} can be rewritten as:
\begin{equation}\label{eqn:CostPerPerformanceBalance}
\begin{split}
f_{cp} = P_{cpu}W_{cpu} + \frac{1}{\beta_{io}}P_{cpu}W_{cpu} + \frac{1}{\beta_{mem}}P_{cpu}W_{mem} \\
+ \beta_{io}P_{io}W_{cpu} + P_{io}W_{io} + P_{io}W_{mem}\\
+ \beta_{mem}P_{mem}W_{cpu} + P_{mem}W_{io} + P_{mem}W_{mem}
\end{split}
\end{equation}
Assuming a CPU core cannot perform disk and memory operation at the same time, partial differentiation with respect to $\beta_{io}$ and $\beta_{mem}$ can separately lead us to the optimum system balance in terms of I/O bandwidth and DRAM size respectively, with respect to processing speed.  

Partially differentiating with respect to $\beta_{io}$, we get:
\begin{equation} \label{eqn:DiffWithBetaIo}
\frac{\partial f_{cp}}{\partial \beta_{io}} = -\frac{1}{\beta_{io}^2}P_{cpu}W_{io} + P_{io}W_{cpu}
\end{equation}
For the optimal balance ($\beta_{io}^{opt}$) between CPU speed and I/O bandwidth, Equation-\ref{eqn:DiffWithBetaIo} should equal to $0$. Then,  solving for $\beta_{io}$ and replacing it with the workload and technology-cost balance terms mentioned in Table-\ref{tab:Notation} we get:
\begin{equation} \label{eqn:BalancedOptIo}
\beta^{opt}_{io} = \sqrt{\frac{\gamma_{io}}{\delta_{io}}}
\end{equation}
Similarly, the optimum balance ($\beta_{mem}^{opt}$) between CPU speed and size of DRAM can be derived as: 
\begin{equation} \label{eqn:BalancedOptMem}
\beta^{opt}_{mem} = \sqrt{\frac{\gamma_{mem}}{\delta_{mem}}}
\end{equation}
Equation-\ref{eqn:BalancedOptIo} and \ref{eqn:BalancedOptMem} show the contribution of application balance and cost balance towards  optimal system balance. 

\subsection{Observations and Inferences}
Gray's law is a special case of our model when the cost balance equals the inverse of the application balance (i.e., the application's CPU  I/O and, memory requirement exactly balance the contemporary cost of the hardware).

Figure-\ref{fig:BetaVsGamma} shows the optimal system balance as a function of application balance and cost balance as proposed by our model. It also compares our model with Amdahl's second law and Gray's law. The horizontal X-axis shows the different types of application balance. value of $\gamma_{io}=1$  presents an I/O as well as CPU-intensive application where Gray's I/O law and Amdahl's I/O law both intersect. Likewise, $\gamma_{mem}=1$ presents a memory and CPU-intensive applications. 

It can be observed that our model sugests to use less DRAM than suggested by Gray's memory law. However, our model yields higher values for system's I/O bandwidth comparing to Gray's I/O law. This is because current price of magnetic disk or SSD is much lower than that of DRAM. 

It can be noticed that lower balance ratio between per-GBPS-I/O-cost to per-GHz-CPU-cost  leads to higher balance ratio between system-I/O-GBPS  to system-CPU-GHz.
One straight forward interpretation for this observation is that if per-GBPS-I/O-cost starts decreasing faster than the per-GHz-CPU-cost, designers need to increase the system-I/O-GBPS of a single server to achieve the new I/O balance ratio (GBPS/GHz). However, Instead of scaling up a single server in terms of storage, designers can scale out in terms of processor. That is, reduce the number of processor in a single server and add more servers with the same new system balance ratio. Theoretically, both these scale-up and scale-out cluster are optimally balanced, i.e., both scale-out and scale-up can provide optimal cost/performance.

A small example scenario can clarify the above concept. Let us consider a fictitious cluster (say, $Cluster^{1}$). Each node of this cluster has $R_{cpu}^{1}$ CPU-speed with $R_{io}^{1}$ I/O bandwidth. Each node of $Cluster^{1}$ is optimally balanced so that it can minimize cost/performance. Let us consider, the optimal I/O balance ratio equals $\beta_{io}^{1}$. Since, each node of $Cluster1$ is optimally balanced, we can say, $\beta_{io}^{1}=\frac{R_{io}^{1}}{R_{cpu}^{1}}$. Now, let us consider for faster price fall for I/O bandwidth than for processor speed, the new optimal system balance raised by $n$ times. That is, the current optimal system balance is $n\beta_{io}^{1}$. To maintain this new optimal system balance, designers have two alternatives:
\begin{itemize}
\item They can straight forward scale up in terms of storage. That is, each node in the new cluster has $R_{cpu}^{1}$ CPU speed  with $nR_{io}^{1}$ I/O bandwidth. Let us denote this cluster as $Cluster^{1}_{scale-up}$
\item Alternatively, they can scale out in terms of processor. That is, each node now has $\frac{R_{cpu}^{1}}{n}$ CPU speed and there are $n$ more nodes in the cluster. Let us denote this cluster as $Cluster^{1}_{scale-out}$
\end{itemize}
It is worth noticing that both $Cluster^{1}_{scale-up}$ and  $Cluster^{1}_{scale-out}$ now has same total aggregated I/O bandwidth, same amount of processing speed. At the same time, Both of them are theoretically maintaining the new optimal system balance ratio ($n\beta_{io}^{1}$) to maximize the cost/performance. 

Similar example can be given for the system's memory balance also. Hence, an implication of our model is that both scale-out and scale-up can theoretically optimize the cost/performance if the cluster is properly balanced.
 
%As soon as the ratio between per GBPS I/O bandwidth cost and per GHz processor speed cost becomes more than 50 our model produces similar result as Gray's law for the I/O- and CPU-intensive applications.

\subsection{Notes on Different types of I/O}
%For an improved processor micro architecture operated at same core speed (such as, Intel Haswell comparing to Intel Xeon), the cost is likely to be higher. That is, $\delta_{io}$ and $\delta_{mem}$ will decrease with a corresponding increase in $\beta_{io}$ and $\beta_{mem}$, thus driving the need for more I/O bandwidth and DRAM size. However, the growth is limited by the square root.
\subsubsection{Sequential vs Random I/O}
It should be noted that the model does not consider sequential and random I/O bandwidth separately. Designers need to be careful about the application characteristics. An application with frequent random I/O may find it beneficial to use SSD instead of HDD to align with the optimum I/O balance (i.e., $\beta_{io}$) produced by the model. For example, Hadoop involves a huge amount of random I/O  in its shuffle phase. To provide such a huge random I/O bandwidth, SSD can be used. 

An underlying assumption of the model is that the aggregate I/O bandwidth of all the disks attached to a compute node is less than or equal to the bandwidth of the disk controllers. Otherwise, the disk controllers can be saturated (an observation by Szalay \cite{cluster:AmdahlBalancedBlade} and our previous work \cite{scaleupscaleout:das2015evaluating}). Because of lower I/O bandwidth of HDD, traditionally, disk controllers have never been the source of a bottleneck. However, the demand for I/O bandwidth is increasing for big data applications. To meet this demand, it is common to  use multiple disks per node. Furthermore, the recent SSD technologies started producing multiple GBPS of I/O bandwidth. It is the designer's responsibility to choose correct hardware so that disk and the disk controller is balanced (i.e., the disk controller is neither overprovisioned nor underprovisioned). 

\subsubsection{Network I/O}
The model does not consider the impact of network interconnect. Hence, we find it important to provide some qualitative information regarding current state of the network architecture in the HPC cluster, source of architectural imbalance, and possible solution. Traditional HPC clusters use a hierarchy of InfiniBand switches with a fat tree model of connectivity. This approach typically uses a layered architecture. The hosts or servers are connected to the bottom layer, called access layer, which is then aggregated to an intermediate layer of switches, called edge layer or leaf. The edge layer switches are then connected to the top layer switches, called core layer or spine, where the actual bandwidth of the network is scaled. To prevent over subscription, the link speeds get progressively higher from access layer to leaves to spine starting from only few Mbps to several Gbps. This architecture may suffer from bottleneck issues, introducing architectural imbalance as the bandwidth of the host adapter is increasing at an outstanding pace (an observation by Cohen \cite{Balance:cohen2009applying}). Furthermore, to accomodate many servers with fewer switches (thus, reducing the cost of the network), a blocking is used, which again limits the performance of big data genomic applications which demand more bandwidth. As an alternative, the simple Clos-based architecture is gaining popularity, where all the lower layer switches (i.e., leaf) are connected to all the top layer switches (i.e., spine) using a full mesh topology, thus achieving a non-blocking model using inexpensive devices. The data-intensive applications based on Hadoop or Giraph, which transmit huge amounts of data over network in different phases of computation, can use more bandwidth from this all-to-all connection model. 

\subsection{An Illustrative Example of Applying the Model to Build a Balanced Cluster}
\begin{table}[!t]
\caption{Cost of different hardware components}
\label{tab:Cost}
\centering
\begin{tabular}{|p{4cm}|c|c|}
\hline
Hardware components & Cost(\$)\tablefootnote{Sample costs are collected from amazon.com, newegg.com, ark.intel.com.} & Unit Price\\
\hline
Intel Xeon 64bit 2.6 GHz E5 series (8-cores) processor & 1760 & \$42/GHz\\
\hline
Intel Xeon D-1541 & 650 & \$54/GHz\\
\hline
\hline
Western Digital RE4 HDD (120MBPS), 500GB & 132 & \$2.25/GBPS/GB\\
\hline
Western Digital VelociRaptor HDD (150MBPS), 600GB & 167 & \$1.90/GBPS/GB\\
\hline
Samsung 840Pro Series SATAIII SSD (400MBPS), 512GB & 450 & \$2.25/GBPS/GB\\
\hline
Samsung NVMe SSD PM953 (2GBPS), 950GB & 450 & \$0.22/GBPS/GB\\
\hline
\hline
Samsung DDR3 16GB memory module & 159 & \$10/GB\\
\hline
32GB 1600MHz RAM (decided by Dell) & 140 & \$4.37/GB\\
\hline
\end{tabular}
\end{table}
\begin{table*}[!t]
\caption{Experimental Testbeds}
\label{tab:Testbeds}
\centering
\begin{tabular}{|p{1.8cm}|p{2cm}|p{1cm}|p{1cm}|p{1cm}|p{1cm}|p{1cm}|p{1cm}|p{1cm}|p{1cm}|p{0.6cm}|p{0.6cm}|}
\hline
Cluster name & Processor & CPU Core Speed (GHz) & \#Cores /node & CPU Speed (GHz) /node & \#Disks /node and type & Seq I/O Bandwidth /Disk (GBPS) & Total Seq I/O Bandwidth (GBPS) /node & DRAM /node (GB) & Maximum \#Nodes available & $\beta_{io}$ & $\beta_{mem}$\\
\hline
SuperMikeII (Traditional Supercomputer) & Two 8-core SandyBridge Xeon & 2.6 & 16 & 41.6 & 1-HDD (SATA) & 0.15 & 0.15 & 32GB & 128 & 0.003 & 0.77\\
\hline
SwatIII (Regular Datacenter) & Two 8-core SandyBridge Xeon & 2.6 &  16 & 41.6 & 4-HDD (SATA) & 0.15 & 0.60 & 256 & 16 & 0.015 & 6.15\\
\hline
CeresII (MicroBrick-based Hyperscale System) & One 6-core Xeon & 2 & 6 & 12 & 1-SSD (NVMe) & 2 & 2 & 64 & 40 & 0.17 & 5.33\\
\hline
\end{tabular}
\end{table*}
In this section, we demonstrate an example of how to apply our model to build a cost-effective, balanced cluster. To reflect today's data-, compute-, and memory-intensive scientific applications, we consider the work done by the CPU, I/O and memory subsystem (i.e., $W_{cpu}$, $W_{io}$, and $W_{mem}$) are equal for that application. That is, using the notation in Table-\ref{tab:Notation}, the application balance  can be written as:
\begin{equation} \label{eqn:PractAppBal}
\gamma_{io} = \gamma_{mem} = 1
\end{equation}

Table-\ref{tab:Cost} shows a sample list of different hardware components and their corresponding cost\footnote{A detail analysis of cost trend is beyond the scope of this paper. However, the samples can reflect the current trend well. Also, the process described in this example can be used for any range of hardware components}. Some of these hardware components are actually used in our experimental testbeds discussed in Section-\ref{sec:ExperimentalTestbed}. Hence, the optimal system balance derived in this section are always referred as a real optimum value throughout the paper. 

The \textit{Unit Price} shows the current price trend  for different processor, storage and memory alternatives in their corresponding unit. We consider Intel Xeon processor. As it can can be seen in  Table-\ref{tab:Cost}, the cost per MBPS of sequential I/O for both HDD and SATA-SSD is almost similar irrespective of change in storage technology provided the same storage space per disk. Whereas, the I/O bandwidth cost started reducing significantly with NVMe SSD. The cost per GB of DRAM is increased almost double from DDR2 to DDR3. 

We first calculated the average cost of each hardware component from the available list (Table-\ref{tab:Cost}) in terms of their corresponding unit price. For example, we have two different Xeon processors, E5-series and D-series as shown in Table-\ref{tab:Cost}. Their rspective unit prices are \$42/GHz and \$54/GHz. Hence, in this example we selected average unit cost of the processor as \$48/GHz. Similarly the cost of DRAM is calculated as \$7.2/GB. 

However, calculating the average I/O bandwidth cost is not as simple as with processor or memory. This is because, the price of the storage devices is often directed by the capacity (i.e., GB), whereas, the model needs the cost per bandwidth (i.e., GBPS) which again depends on the number of disks. Hence, we need a general methodology to calculate cost per GBPS. To resolve this issue, we first calculated the unit price of a disk in terms of cost per GBPS per GB. Then, we selected a common capacity, 1TB for this example that we can use in a server.  Then, we multiplied the unit price with capacity (i.e., 1TB) to calculate the cost per GBPS for each of the disks. For example, the cost per GBPS of Western Digital RE4 HDD is \$2304/GBPS ($2.25 \times 1024$). Finally, we calculated the average cost of I/O bandwidth for all the disks listed. For the disk drives shown in Table-\ref{tab:Cost}, the average I/O bandwidth cost is \$1694/GBPS. 

Next, we divided the I/O cost per GBPS  and DRAM cost per GB by the CPU cost per GHz to get the cost balance for I/O and memory respectively. Using the notation in Table-\ref{tab:Notation}, cost balance for this example can be written as: 
\begin{equation} \label{eqn:PractCostBalIo}
\delta_{io} = 35.30 
\end{equation}
\begin{equation} \label{eqn:PractCostBalMem}
\delta_{mem} = 0.15
\end{equation}
Using Equation-\ref{eqn:BalancedOptIo} and replacing the value of $\gamma_{io}$ and $\delta_{io}$ from Equation-\ref{eqn:PractAppBal} and \ref{eqn:PractCostBalIo} respectively, we can get the system's I/O balance in terms of GBPS/GHz as:
\begin{equation} \label{eqn:PractSysBalIo}
\beta_{io}^{opt} = 0.17
\end{equation}
Similarly, using Equation-\ref{eqn:BalancedOptMem}, \ref{eqn:PractAppBal}, and \ref{eqn:PractCostBalMem}, we can get the system's DRAM balance in terms of GBPS/GHz as:
\begin{equation} \label{eqn:PractSysBalMem}
\beta_{mem}^{opt} = 2.7
\end{equation} 

\section{Experimental Testbeds: Cluster Characterization Using Proposed Model} \label{sec:ExperimentalTestbed}
As mentioned earlier, we evaluate three different cluster architectures. Table-\ref{tab:Testbeds} shows the overview of our experimental testbeds. The first one, SuperMikeII represents a traditional HPC cluster. This LSU HPC cluster offers a total of 440 computing nodes. However, a maximum 128 can be allocated at a time to a single user. SwatIII represents a regular datacenter. This Samsung datacenter has 128 nodes. However, we used maximum 16 nodes for our experiments. The last one, CeresII, is a novel hyperscale system, based on Samsung MicroBrick with a maximum of 40 nodes available to us.

\subsection{Cluster Characterization System's I/O and Memory Balance}
%Figure-\ref{fig:beta_io} and \ref{fig:beta_mem} shows the change in system's optimum I/O and memory balance respectively with respect to Intel Xeon processor architecture for varying application balance (i.e., application's resource requirement). Based upon this figure characterize each of our experimental testbeds on the basis of $\beta_{io}$ and $\beta_{mem}$ and discuss the major pros and cons of each different architecture.  

%In this section we characterize each cluster on the basis of system balance parameters that we derived earlier and discuss the major pros and cons of each different cluster architecture. Since all the clusters in our evaluation use the same processor family (i.e., Intel Xeon) we use Equation-?? and ?? to calculate the Amdahl’s I/O and memory numbers of different clusters.
\subsubsection{SuperMikeII (Traditional HPC cluster)}
SuperMikeII has two 8-core Intel Xeon E5 series processor per node thus offering huge processing power. However, each SuperMikeII node is equipped with only one HDD (Western Digital RE4), thus limited in terms of I/O bandwidth. Also, each SuperMikeII node has only 32GB DRAM (Dell). As a result, SuperMikeII has $\beta_{io}=0.003$ and $\beta_{mem}=0.77$, both a magnitude smalller than the optimum produced by our model for a data-, compute- and memory-intensive application as shown in Equation-\ref{eqn:BalancedOptIo} and Equation-\ref{eqn:BalancedOptMem}. Using the plot shown in Figure-\ref{fig:BetaVsGamma} (or using Equation-\ref{eqn:BalancedOptIo} and \ref{eqn:BalancedOptMem} with SuperMikeII hardware cost shown in Table-\ref{tab:Cost}) SuperMikeII provides optimal cost/performance for those applications where $\gamma_{io}=0.0005$ or $\gamma_{mem}=.06$. Hence, it can be said SuperMikeII can provide cost-optimized performance for traditioal compute-intensive applications  such as supercomputing simulations, astrophysics calculations where $\gamma_{io}$ is in the order of $10^{-3}$.  
\subsubsection{SwatIII (Existing Datacenter)}
Unlike SuperMikeII which has only one HDD per node, SwatIII uses 4HDDs (Western Digital VelociRaptor) per node using JBOD (Just a Bunch of Disk) configuration while using the same processors (i.e. two 8core Intel Xeon E5 series) as SuperMikeII. Since the I/O throughput increases linearly with the number of disks, SwatIII's $\beta_{io}=0.015$ is higher than SuperMikeII but lower than the optimum produced by our model for an I/O- and compute-intensive application (Equation-\ref{eqn:BalancedOptIo}). On the other hand, each SwatIII node has 256GB DRAM (Samsung DDR3), thus achieving a very high value for $\beta_{mem}=6.15$. It is worth noticing that $\beta_{mem}$ of SwatIII is even higher than the optimum produced by the model (Equation-\ref{eqn:BalancedOptMem}). Using Figure-\ref{fig:BetaVsGamma} (or using Equation-\ref{eqn:BalancedOptIo} and \ref{eqn:BalancedOptMem} with SuperMikeII hardware cost shown in Table-\ref{tab:Cost}), we can show SwatIII can produce cost optimized performance when $\gamma_{io} = 0.01$ and $\gamma_{mem}=1.47$. That is, SwatIII can be a good choice for moderately I/O-intensive applications and for memory-intensive applications such as in-memory NoSQL. However, SwatIII may show worse cost/performance for many of the modern I/O-intensive big data applications. 
\subsubsection{CeresII (MicroBrick-based Hyperscale System)}
The last one, CeresII, is a novel hyperscale system based on Samsung MicroBricks. Each MicroBrick (or simply a compute server) of CeresII consists of a 6core Intel Xeon D-1541 processor with a core frequency of 2GHz, one NVMe-SSD (Samsung PM953) with an I/O bandwidth of 2GBPS, and 64GB DRAM (Samsung DDR3). $\beta_{io}$ of CeresII is $0.17$ which is same as the optimum calculated by our model in Equation-\ref{eqn:BalancedOptIo}. On the other hand, $\beta_{mem}$ of each CeresII module is $5.33$. Although, it is higher than the optimal, it is less than SwatIII. Thus, CeresII is the most balanced cluster among all the available resources and we expect to get the best cost to performance for today's I/O-, compute- and memory-intensive applications. 
\begin{table*}
\caption{Data size for different benchmark applications}
\label{tab:AppChar}
\begin{center}
    \begin{tabular}{ |p{3cm} | p{1.5cm} | p{2.5cm} | p{2.5cm} | p{0.8cm} | p{0.8cm} | p{0.8cm} | p{2.5cm}|} \hline
     Job name & Job Type & Input & Final output & \# jobs & Shuffled data & HDFS Data & Application Characteristics \\ \hline
    Terasort & Hadoop & 1TB & 1TB & 1 & 1TB & 1TB & Map: CPU-intensive, Reduce: I/O-intensive \\ \hline
    Wordcount & Hadoop & 1TB & 1TB & 1 & 1TB & 1TB & Map and Reduce: CPU-Intensive \\ \hline 
   %Graph Construction (moderate size bumble bee genome)  & Hadoop & 90GB (500-million reads) & 95GB & 2 & 2TB & 136GB & Map and Reduce: CPU- and I/O-intensive \\ \hline
    %Graph Simplification (moderate size bumble bee genome) & Series of Giraph jobs & 95GB (71581898 vertices) & 640MB (62158 vertices) & 15 & - & 966GB & Memory-intensive\\ \hline    
    Graph Construction (large size human genome) & Hadoop & 452GB (2-billion reads) & 3TB & 2 & 9.9TB & 3.2TB &Map and Reduce: CPU- and I/O-intensive\\ \hline
    Graph Simplification (large size human genome) & Series of Giraph jobs & 3.2TB (1483246722 vertices) & 3.8GB (3032297 vertices) & 15 & - & 4.1TB & Memory-Intensive \\ \hline 
    \end{tabular}
\end{center}
\end{table*}
\section{Cluster Evaluation Methodology} \label{sec:EvalMethod}
%To evaluate the clusters shown in Table-\ref{tab:Testbeds}, we use SuperMikeII as the baseline configuration and compare the performance of other clusters with respect to this. We always use homogeneous configuration across any cluster. One of the nodes in each cluster configuration is always used as Hadoop NameNode. All other nodes are used as DataNodes.

\subsection{Software platform: Hadoop and Giraph}
To evaluate the relative merits of the clusters with respect to data-intensive scientific applications, we first need a big data analytic platform. The obvious selection is Hadoop, which is nowadays the de facto standard for big data analysis and rapidly gaining popularity in scientific computing. We also evaluate the cluster hardware with respect to Giraph, which is a large scale graph processing framework developed atop Hadoop. We use Clouera-Hadoop-2.3.0 and Giraph-1.1.0 for the entire study. 

\subsection{Hadoop Configurations Overview}
For each cluster configuration, one of the nodes is always used as Hadoop NameNode. All other nodes are used as DataNodes. Since our goal is to evaluate the  the architectural balance of different hardware components, we avoid any unnecessary changes in the source code of Hadoop or Giraph. To evaluate the balance between the hardware components, we set up the Hadoop parameters such that the application can make use of the maximum available processing speed, I/O bandwidth and DRAM. However, it is worthy to mention here that due to limited I/O bandwidth in SuperMikeII (traditional HPC cluster with only 1HDD per node) we could launch only 8 concurrent YARN containers (i.e., 8 concurrent map/reduce tasks), which is half of the total number of cores to get the optimal performance. Appendix-\ref{app:HadoopConfigurationsAndoptimizations} shows a brief description of our Hadoop configurations.

\subsection{Benchmark Application Characteristics}
Table-\ref{tab:AppChar} summarizes the details of the benchmark applications, the data size handled by each of these applications and their corresponding characteristics. We used three benchmark applications for our evaluation: TeraSort, WordCount, and a real world genome assembly application. A brief description of each of these applications are as follows:

\subsubsection{TeraSort}
The first one is TeraSort, a standard benchmark for any data processing framework. The Hadoop version of TeraSort samples and partitions the input data in its map phase based upon only first few characters. The reduce phase then uses the quicksort algorithm to sort each of the partitions locally. 
The map phase of TeraSort is CPU-intensive as it reads only the first few characters of each row in the input dataset to generate the key (called sample-key) for each data. However, based upon the data size, the reduce phase can be severely I/O-intensive. The entire dataset needs to be transferred to different reducers, and each reducer needs to read each row at least once to sort the data set. Again, for huge volumes of data partitions, that cannot fit in memory, an external sort may be applied making the reduce phase severely I/O intensive.  

\subsubsection{WordCount}
The second one is WordCount, another widely used Hadoop benchmark. The map phase of WordCount parses the input dataset line by line to extract each word. Each word is emitted with a $1$ as its initial count, which is then summed up in the reduce phase to output its  frequency. 
Since both the map and reduce phase read the entire data set sequentially just once, both phases of WordCount is CPU-intensive. However, it should not be compared with the traditional compute-intensive applications such as simulations. Because of the huge volume of data, WordCount also needs to spend a significant amount of time for I/O.

\subsubsection{Genome Assembly}
The third one is a genome assembly application that we developed based on Hadoop and Giraph to address the challenges in large-scale genome assembly, which recently made its way to the forefront of big data challenges. The first phase uses Hadoop to scan the genome data set to filter the lines containing only neucleotide characters (i.e., $A$, $T$, $G$, and $C$) and then divides each of those lines into smaller fragments of length $k$, known as $k$-mer. The map phase emits each two subsequent $k$-mers as a key-value pair representing a vertex and an edge from it. The reduce phase then aggregates all the edges emitted from each vertex to write the entire graph structure to HDFS. The second phase of the assembly application uses Giraph to to compress each of the linear chains in the graph structure to one vertex and remove the tip and bubble structures in the graph (included because of sequencing error). After removing the tips and bubbles the graph may have some new linear chains which are compressed again. Similarly, new tip and bubble structures are removed again. So, the process continues incrementally as a series of Giraph jobs until there are no tips or bubbles in the graph. Appendix-\ref{app:GenomeAssemblyAlgorithms} shows the overview of the algorithms used in our genome assembly application.

\subsection{Data Size}
For both TeraSort and WordCount, we generated 1TB random dataset as the input for these benchmarks. Both TeraSort and WordCount generate almost a similar amount data in the intermediate shuffle phase. The output data size of TeraSort is also same as its input (i.e., 1TB in this work), whereas the output of WordCount may vary based upon the frequency of different words in the randomly generated dataset. However, it is closed to 1TB.

For the genome assembly benchmark application, we use a large size human genome dataset (452GB). The corresponding graph size is 3.2TB. The Hadoop stage of the assembly application is severely shuffle intensive. The temporary shuffle data is almost 21-times more than the input size. The Human genome is openly available in NCBI website with accession number SRX016231\footnote{http://www.ncbi.nlm.nih.gov/sra/SRX016231[accn]}.

\section{Result and Discussion} \label{sec:Result}
\subsection{Evaluating the Cost/Performance of Different Clusters with TeraSort and WordCount}
\begin{table}[!t]
\caption{Resources available for each cluster architecture when scaling with respect to cost. The cost of 16 nodes of SuperMikeII is used as baseline. This is used for TeraSort and WordCount benchmark}
\label{tab:ScalingCost}
\label{fig:perf}
\centering
\begin{tabular}{|p{2.5cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|} \hline
Cluster Configurations & SuperMikeII & SwatIII & CeresII\\ \hline
Total cost (\$) & 60864 & 60864 & 60864\\ \hline
Cost/nodes (\$) & 3804 & 6911 & 2700\\ \hline
\#Nodes & 16 & 9 & 23\\ \hline
Total processing speed (GHz) & 624.00 & 330.83 & 300.00\\ \hline
Total I/O bandwidth (GBPS) & 2.25 & 4.80 & 50.00\\ \hline
Total storage space (TB) & 7.50 & 16.00 & 25.00\\ \hline
Total DRAM Size (TB) & 0.48 & 2.00 & 1.60\\ \hline
\end{tabular}
\end{table}
To show the balance between performance and economy, we keep the total cost\footnote{The cost information of each hardware component can be found in Table-\ref{tab:Cost}} the same across all the clusters. Table-\ref{tab:Cost} shows the available resources for all three cluster architectures available to us (i.e. SuperMikeII, SwatIII and CeresII) while keeping the total cost same across all the clusters. We used the cost of $16$ nodes of SuperMikeII as the baseline. Then, we divided this baseline-cost by the cost of each node in SwatIII and CeresII to count the number of nodes to be used in these two different clusters.

We ran the first two applications described in Table-\ref{tab:AppChar} in all three cluster configurations and measured their execution time. All results are the means of at least 3 runs of each application on each configuration. Figure-\ref{fig:perf} shows the results normalized to the SuperMikeII baseline. That is, the  execution time of SuperMikeII is always $1$. We see that CeresII, being closer to the optimum produced by our model performs surprisingly well: 
\begin{enumerate}
\item Comparing to the SuperMikeII baseline, for both TeraSort and WordCount benchmark application, CeresII shows almost 65\% improvement. 
\item Comparing to SwatIII, CeresII shows almost 50\% improvement in execution time for TeraSort and WordCount. 
\end{enumerate}
\begin{figure}[!t]
\centering
\includegraphics[width=.5\textwidth]{Figures/PerformanceFigures/execTimeTnW.pdf}
\caption{Execution time of TeraSort and WordCount over different cluster architecture keeping the total cost of each cluster same}
\end{figure}
\subsubsection{System Characteristics}
\begin{figure}[htb]
	\begin{subfigure}[b]{0.23\textwidth}
                \includegraphics[width=\textwidth]{Figures/SystemFigures/TeraSortIPS.pdf}
                \caption{TeraSort IPS}
                \label{fig:TeraSortIPS}
    \end{subfigure}
 	\begin{subfigure}[b]{0.23\textwidth}
                \includegraphics[width=\textwidth]{Figures/SystemFigures/WordCountIPS.pdf}
                \caption{WordCount IPS}
                \label{fig:WordCountIPS}
   \end{subfigure}
   %\begin{subfigure}[b]{0.24\textwidth}
   %             \includegraphics[width=\textwidth]{Figures/SystemFigures/BombusGrConsIPS.pdf}
   %             \caption{Bumble bee Hadoop}
   %             \label{fig:BombusGrConsIPS}
   % \end{subfigure}
 	%\begin{subfigure}[b]{0.24\textwidth}
    %            \includegraphics[width=\textwidth]{Figures/SystemFigures/BombusGrSimpIPS.pdf}
    %            \caption{BumbleBee Giraph}
    %            \label{fig:BombusGrSimpIPS}
   %\end{subfigure}
   
   	\begin{subfigure}[b]{0.23\textwidth}
                \includegraphics[width=\textwidth]{Figures/SystemFigures/TeraSortIO.pdf}
                \caption{TeraSort I/O throughput}
                \label{fig:TeraSortIO}
    \end{subfigure}
 	\begin{subfigure}[b]{0.24\textwidth}
                \includegraphics[width=\textwidth]{Figures/SystemFigures/WordCountIO.pdf}
                \caption{WordCount I/O throughput}
                \label{fig:WordCountIO}
   \end{subfigure}
   \caption{System Characteristics of TeraSort and WordCount benchmark on different cluster architectures}
  \label{fig:IPS}
\end{figure}
%\begin{figure}[htb]
%	\begin{subfigure}[b]{0.23\textwidth}
%               \includegraphics[width=\textwidth]{Figures/SystemFigures/TeraSortIO.pdf}
%                \caption{TeraSort}
%                \label{fig:TeraSortIO}
%    \end{subfigure}
% 	\begin{subfigure}[b]{0.24\textwidth}
%                \includegraphics[width=\textwidth]{Figures/SystemFigures/WordCountIO.pdf}
%                \caption{WordCount}
%                \label{fig:WordCountIO}
%   \end{subfigure}
   %\begin{subfigure}[b]{0.24\textwidth}
   %             \includegraphics[width=\textwidth]{Figures/SystemFigures/BombusGrConsIO.pdf}
   %             \caption{Bumble bee Hadoop}
   %             \label{fig:BombusGrConsIO}
    %\end{subfigure}
 	%\begin{subfigure}[b]{0.24\textwidth}
    %            \includegraphics[width=\textwidth]{Figures/SystemFigures/BombusGrSimpIO.pdf}
    %            \caption{BumbleBee Giraph}
    %            \label{fig:BombusGrSimpIO}
   %\end{subfigure}
%   \caption{I/O Bandwidth}
%  \label{fig:IO}
%\end{figure}
To monitor the system characteristics we used Perf tool and the Cloudera-Manager-5.0.0. Figure-\ref{fig:IPS} shows the total number of CPU instructions per second (IPS) and I/O bandwidth across different cluster architectures while keeping the total cost of the cluster the same. 

For both TeraSort and WordCount, the peak IPS of SuperMikeII is significantly higher than both SwatIII and CeresII. However, the average I/O bandwidth of SuperMikeII is significantly less than the other two, leading to significantly high I/O wait that results in lower average IPS. This huge difference between peak and average IPS is because of the extremely low value of $\beta_{io}$ in SuperMikeII.

SwatIII cluster shows a better balance between CPU and I/O than SuperMikeII. However, because of its high size of DRAM, it gives up processing speed with respect to CeresII (128 core in SwatIII vs 150 cores in CeresII) when the total cost of the cluster is same. 

CeresII cluster is optimally balnced in terms of CPU speed and I/O bandwidth with a $\beta_{io}$ value of 0.17. Figure-\ref{fig:TeraSortIPS} and \ref{fig:WordCountIPS} shows that CeresII achieves higher Peak IPS than SwatIII but less than SuperMikeII for both the TeraSort and WordCount benchmark. On the other hand, CeresII achieves significantly higher I/O bandwidth than both clusters, keeping the peak and average IPS of the application similar, leading to the lowest execution time for any of the applications.

\subsection{Evaluating Different Cluster for Large Size Human Genome Assembly}
\begin{table}[!t]
\caption{Maximum available resources in each cluster architecture (used for large human genome assembly)}
\label{tab:TotalCluster}
\label{fig:perf}
\centering
\begin{tabular}{|p{2.5cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|} \hline
Cluster Configurations & SuperMikeII & SwatIII & CeresII\\ \hline
Total cost (\$) & 486912 & 110576 & 10800\\ \hline
Cost/node (\$) & 3804 & 6911 & 2700\\ \hline
\#Nodes & 128 & 16 & 40\\ \hline
Total processing speed (GHz) & 5324.8 & 665.6 & 480.00\\ \hline
Total I/O bandwidth (GBPS) & 19.2 & 9.60 & 80.00\\ \hline
Total storage space (TB) & 7.50 & 16.00 & 25.00\\ \hline
Total DRAM Size (TB) & 4 & 4 & 2.5\\ \hline
\end{tabular}
\end{table}
\begin{figure}[htb]
	\begin{subfigure}[b]{0.23\textwidth}
                \includegraphics[width=\textwidth]{Figures/PerformanceFigures/execTimeHum.pdf}
                \caption{Execution time}
                \label{fig:HumPerf}
    \end{subfigure}
 	\begin{subfigure}[b]{0.23\textwidth}
                \includegraphics[width=\textwidth]{Figures/PerformanceFigures/costTimeHum.pdf}
                \caption{Cost/performance }
                \label{fig:HumCost}
   \end{subfigure}
   \caption{Performance of different cluster for large size human genome assembly}
  \label{fig:Hum}
\end{figure}
To assemble the large human genome (452GB), we used the maximum available resources in each of the clusters to accommodate the huge amount of shuffled data (9.9TB) as well as the graph data (3.2TB). That is, we used all 128 nodes of SuperMikeII, 16 nodes of SwatIII, and 40 nodes of CeresII for this application. Figure-\ref{fig:HumPerf} and \ref{fig:HumCost} shows the execution time and the cost/performance respectively for the Hadoop and Giraph stages of the human genome assembly pipeline. As shown in the model, we consider the performance as the inverse of the execution time and multiplied the execution time with the total cost of each cluster to get the corresponding cost/performance. All data is again normalized to SuperMikeII baseline. The results are as follows:
\begin{enumerate}
\item CeresII, even with almost 90\% less processing speed than SuperMikeII across the cluster, outperformed SuperMikeII by more than 30\% in the Hadoop stage of the assembly. In the Giraph stage, the performance gain is almost 20\%. In terms of cost/performance, the corresponding gains are 88\% and 85\% respectively. 
\item Comparing to SwatIII,  the processing power of CeresII is 72\%. However,  due to the optimal architectural balance, CeresII outperforms SwatIII by almost 50\% in the Hadoop stage. The corresponding improvement in the Giraph stage is 20\%. In terms of cost/performance, CeresII shows almost 50\% and 30\% improvement over SwatIII for the Hadoop and Giraph stages respectively.
\end{enumerate}

% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure}

% Note that the IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a 
% line do not exceed the text width or a line break will occur.
%
%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.
% Be aware that for subfig.sty to generate the (a), (b), etc., subfigure
% labels, the optional argument to \subfloat must be present. If a
% subcaption is not desired, just leave its contents blank,
% e.g., \subfloat[].


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table and, given that table
% captions serve much like titles, are usually capitalized except for words
% such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to
% and up, which are usually not capitalized unless they are the first or
% last word of the caption. Table text will default to \footnotesize as
% the IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that the IEEE does not put floats in the very first column
% - or typically anywhere on the first page for that matter. Also,
% in-text middle ("here") positioning is typically not used, but it
% is allowed and encouraged for Computer Society conferences (but
% not Computer Society journals). Most IEEE journals/conferences use
% top floats exclusively. 
% Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the
% \fnbelowfloat command of the stfloats package.

\section{Limitations of the model} \label{sec:Limitations}
As an initial approach, our main focus was on simplicity. To keep the model simple, we gave up several details and micro-architecture in CPU, storage, and memory subsystems. It is always possible to include more subtle parameters, such as CPU multi-threading, number of I/O operations per second and I/O latency, network interconnect across cluster, memory-bandwidth and latency, etc., to the existing additive model to come up with better and more accurate equation for system balance. We also ignored the overlap between work done by CPU and disk which severely depends on the software framework and application's I/O characteristics and its nature of the parallelism (partitioned and pipelined). Building a better, accurate model is the part of our future work.  

\section{Conclusion and Future Work} \label{sec:Conclusion}
Big data needs big resources. As the nature of scientific computing is changing from compute-centric to data-centric, it is obvious that providing more resources (more processing speed, I/O bandwidth, DRAM, etc.) will provide more performance. So, the major challenge is now in  providing expected performance in reduced cost. This paper makes an initial attempt to analytically resolve the performance and cost conundrum prevalent in big data research.

We propose a simple additive model to express the optimal system balance in terms of application characteristics and hardware price to minimize the cost/performance.   We also verified our claim experimentally that the cluster that resembles the optimal produced by our model, (i.e., CeresII) outperforms an existing HPC cluster and a regular data center architecture by several magnitudes for different types and sizes of applications.

The proposed model also has implications for the way current big data HPC clusters or data center clusters are provisioned. Instead of evaluating the clusters with respect to scale-up and scale-out architecture, we argued for a system balance. We theoretically showed that if proper balance is maintained between different hardware components, both scale-out and scale-up clusters can provide optimize the cost/performance.  However, because of resource limitations, we could not validate this claim and, thus, this is obviously one of the future directions of our research.

\appendices
\section{Hadoop and Giraph: Programming model and Configuration}
This part is similar to our previous work \cite{scaleupscaleout:das2015evaluating}.
 \label{app:HadoopConfigurationsAndoptimizations}
\subsection{Programming model of Hadoop and Giraph}
Hadoop and Giraph were originated as the open-source counterpart of Google's MapReduce \cite{fw:mapreduce} and Pregel \cite{fw:pregel} respectively. Both the software read the input data from the underlying Hadoop Distributed File System (HDFS) in the form of disjoint sets or partitions of records. Then, in the MapReduce abstraction, a user-defined map function is applied to each disjoint set concurrently to extract information from each record in the form of intermediate key-value pairs. These key-value pairs are then grouped by the unique keys and shuffled to the reducers. Finally, a user-defined reduce function is applied to the value-set of each key, and the final output is written to the HDFS. The MapReduce framework enables data- and compute-intensive applications to run large volume of distributed data sets over distributed compute nodes with local storage. On the other hand, Giraph uses the Bulk Synchronous Parallel model \cite{fw:bsp} where computation proceeds in supersteps. In the first phase of a superstep, Giraph leverages Hadoop-mappers when a user-defined vertex-program is applied to all the vertices concurrently. In the end of each superstep, each vertex can send a message to other vertices to initiate the next superstep. Alternatively, each vertex can vote to halt. The computation stops when all the vertices vote to halt unanimously in the same superstep. Giraph enables memory- and compute-intensive applications to upload data into distributed memories over different compute nodes.
 
\subsection{Hadoop configuration and optimization} 
\textbf{Number of concurrent YARN containers:} After performing rigorous testing,  we observed, that for SuperMikeII and SwatIII-Basic-HDD (1-HDD/DN cases), 8-conainers/DN (i.e., half of total cores/node) show the best result. For any other cluster, number of concurrent containers per datanode is kept equal to the number of cores per node. 

\textbf{Amount of memory per container and Java-heap-space:} In each node in any node of any cluster, we kept 10\% of the memory for the system use. The rest of the memory is equally divided among the launched containers. The Java heap space per worker is always set to lower than memory per container as per normal recommendation.

\textbf{Total number of Reducers:} Based on the observation of job profiles, we observed that 2-times of reducers than number of concurrent containers produce good performance in general. 

\textbf{Giraph workers:} The number of Giraph workers is set according to the number of concurrent YARN containers.

\textbf{Other Giraph parameters:} We always used enough memory to accommodate the graph structure in memory and always avoided using the out-of-core execution feature of Giraph, which writes huge data to the disk. We also avoided using the checkpoint feature for the same reason.


\section{Genome Assembly Algorithms}
\label{app:GenomeAssemblyAlgorithms}
%\subsubsection{TeraSort}
%TeraSort smaples the input data and uses MapReduce to sort the entire data. The map phase of TeraSort samples and partitions the entire dataset by reading only the first few characters of each row. Then, it assigns a key (called sample-key) to each partition of the input dataset. The sample-keys are generated such that, the sample-key of a partition (say $partition_i$) is less than the sample-key of the next partition (i.e., $partition_{i+1}$). Then,based upon these sample keys the data set is distributed over the reducers. Then each reducer uses quicksort to sort the local partitions of the data assigned to it. and the sorted data is written to the HDFS. 
%\subsubsection{WordCount}
%Hadoop WordCount is a workload that reads an input text file and counts how often a certain word exists in the text file. each line is fed into a mapper as an input and breaks down each line into words. It then uses a key/value pair for each word with $1$ next to each word as initial count. A reducer sums the count of each word and issues a key/value pair with the word and sum for each word and the final output is written to HDFS. 
A similar description of the assembly application can be found in our previous work \cite{scaleupscaleout:das2015evaluating}. For more information on our genome assembly application refer to  

The motivation behind selecting genome assembly application is that, the high throughput next generation DNA sequencing machines (e.g., Illumina Genome Analyzer) has outpaced Moore's law and started producing a huge amount of short read sequences typically in the scale of several Gigabytes to Terabytes. Furthermore, the size of the de Bruijn graph built from these vast amount of short reads may be a magnitude higher than the reads itself making the entire assembly pipe line severely data-intensive.

De novo genome assembly refers to the construction of an entire genome sequence from a huge amount of small, overlapping and erroneous fragments called short read sequences while no reference genome is available. The problem can be mapped as a simplified de Bruijn graph traversal \cite{bio:debruijngraph}. We classified the de novo assembly in two stages as follows:
\begin{inparaenum}[\itshape a\upshape)]
\item Hadoop-based de Bruijn graph-construction and
\item Giraph-based graph-simplification.  
\end{inparaenum}

\subsubsection {Hadoop-based De Bruijn graph-construction (data- and compute-intensive workload)}
After filtering the actual short reads (i.e., the line containing only nucleotide characters $A$, $T$, $G$, and $C$) from a standard fastq file, an extremely shuffle-intensive Hadoop job creates the graph from these reads.  the Hadoop map task divides each read into several short fragments of length $k$ known as $k$-mers. Two subsequent $k$-mers are emitted as an intermediate key-value pair that represents  a vertex and an edge (emitted from that vertex) in the de Bruijn graph.  The reduce function aggregates the edges (i.e the value-list) of each vertex (i.e., the $k$-mer emitted as key) and, finally, writes the graph structure in the HDFS in the adjacency-list format. Based upon the value of $k$ (determined by biological characteristics of the species), the job produces huge amount of shuffled data. For example, for a read-length of 100 and $k$ of 31 the shuffled data size is found to be 20-times than the original fastq input. On the other hand, based upon the number of unique $k$-mers, the final output (i.e., the graph) can vary from 1 to 10 times of the size of the input. 

\subsubsection {Giraph-based Graph Simplification (memory- and compute-intensive workload)}
The large scale graph data structure produced by the last MapReduce stage is analyzed here. This stage consists of a series of memory-intensive Giraph jobs. Each Giraph job consists of three different types of computation: compress linear chains of vertices followed by removing the tip-structure and then the bubble-structure (introduced due to sequencing errors) in the graph. Giraph can maintain a counter on the number of supersteps and the master-vertex class invokes each type of computation based on that.  

We compress the linear chains into a single vertex using a randomized parallel list ranking algorithm of \cite{algo:parallellistrank} implented with Giraph. The computation proceeds in rounds of two supersteps until a user defined $limit$ is reached. In one superstep, each compressible vertex with only one incoming and outgoing edge is labeled with either $head$ or $tail$ randomly with equal probability and send a message containing the tag to the immediate predecessor. In the next superstep, all the $head$-$tail$ links are merged, that is, the $head$-$k$mer is extended (or, appended) with the last character of the $tail$-$k$mer and the $tail$ vertex is removed. Each vertex also maintain a frequency counter which increments after each extension to that vertex.After the compression, all the tip-structures in the graph are removed in two supersteps. The first superstep identifies all the vertices with very short length (less than a threshold) and no outgoing edge as tips which are removed in the second superstep.Finally, the bubbles-structures are resolved in another two supersteps. In the first superstep, the vertices with same predecessor and successor as well as very short length (less than a threshold) are identified as bubbles. They send a message containing their id, value, and frequency to their corresponding predecessors. The predecessor employs a Levenshtein-like edit distance algorithm. If the vertices are found similar enough, then the lower frequency vertex is removed. 

% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%


%\appendices
%\section{Proof of the First Zonklar Equation}
%Appendix one text goes here.

% you can choose not to have a title for an appendix
% if you want by leaving the argument blank
%\section{}
%Appendix two text goes here.


% use section* for acknowledgment
\section*{Acknowledgment}
%The authors would like to thank...
This work has been supported in part by the NSF CC-NIE grant (NSF award \#1341008).

% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
\bibliography{bare_jrnl_bib}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
%\begin{thebibliography}{1}

%\bibitem{IEEEhowto:kopka}
%H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
 % 0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.

%\end{thebibliography}

% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

%\begin{IEEEbiography}{Michael Shell}
%Biography text here.
%\end{IEEEbiography}

% if you will not have a photo at all:
%\begin{IEEEbiographynophoto}{John Doe}
%Biography text here.
%\end{IEEEbiographynophoto}

% insert where needed to balance the two columns on the last page with
% biographies
%\newpage

%\begin{IEEEbiographynophoto}{Jane Doe}
%Biography text here.
%\end{IEEEbiographynophoto}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}



% that's all folks
\end{document}